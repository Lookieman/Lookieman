{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lookieman/home_projects/blob/main/ThinkOnwardsComp%5CPhase2d_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwD8FgH1iqy_"
      },
      "source": [
        "**Phase** 2d now tries to focus on improvement of performance by:\n",
        "\n",
        "\n",
        "\n",
        "*   Tweaking the parameters for teh Cosine Annealing LR\n",
        "*   Adding Sprectral Norm for Stabilized Unet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRVNO74deCo7",
        "outputId": "dfd83c92-7048-4ceb-b0ec-aa950e04835d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Phase2a\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from scipy import ndimage\n",
        "from math import e\n",
        "\n",
        "#phase 2b\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#phase 2c\n",
        "import random\n",
        "\n",
        "#phase 2d\n",
        "from torch.nn.utils import spectral_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JhRM7dE45Hm",
        "outputId": "2667dcfd-04c1-45d8-caba-be15e563db45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "USE_ADAPTIVE_MODEL = True  # Set to True for adaptive model\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 35\n",
        "ENSEMBLE_SEEDS = [42, 123, 456]  # 3 seeds per fold\n",
        "K_FOLDS = 5\n",
        "EARLY_STOPPING_PATIENCE = 8\n",
        "GRADIENT_CLIP_NORM = 0.5\n",
        "DROPOUT=0.05\n",
        "\n",
        "\n",
        "\n",
        "data_dir = Path('/content/drive/MyDrive/ThinkOnward/Data/Train')\n",
        "result_dir = Path('/content/drive/MyDrive/ThinkOnward/Result/Phase2c')\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "#print(f\"Model type: {'Adaptive' if USE_ADAPTIVE_MODEL else 'Resize'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns9AknqeWMaA"
      },
      "outputs": [],
      "source": [
        "def setup_deterministic_training(seed):\n",
        "    \"\"\"Setup completely deterministic training environment\"\"\"\n",
        "\n",
        "    # Set all random seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensure deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Enable deterministic algorithms where possible\n",
        "    try:\n",
        "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "    except:\n",
        "        pass  # Not all PyTorch versions support this\n",
        "\n",
        "    print(f\"✅ Deterministic training setup complete with seed {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6lOdwsc48nx"
      },
      "outputs": [],
      "source": [
        "class SeismicDataset(Dataset):\n",
        "    def __init__(self, data_dir, sample_indices, use_adaptive=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.sample_indices = sample_indices\n",
        "        self.use_adaptive = use_adaptive\n",
        "        self.receiver_files = [\n",
        "            'receiver_data_src_1.npy',\n",
        "            'receiver_data_src_75.npy',\n",
        "            'receiver_data_src_150.npy',\n",
        "            'receiver_data_src_225.npy',\n",
        "            'receiver_data_src_300.npy'\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_idx = self.sample_indices[idx]\n",
        "        sample_dir = os.path.join(self.data_dir, f'TrainingData_{sample_idx}')\n",
        "\n",
        "        # Load receiver data (5 files)\n",
        "        receiver_data = []\n",
        "        for file_name in self.receiver_files:\n",
        "            file_path = os.path.join(sample_dir, file_name)\n",
        "            data = np.load(file_path).astype(np.float32)  # (10001, 31)\n",
        "            receiver_data.append(data)\n",
        "\n",
        "        # Load target velocity model\n",
        "        target_path = os.path.join(sample_dir, 'vp_model.npy')\n",
        "        target = np.load(target_path).astype(np.float32)  # (300, 1259)\n",
        "\n",
        "        if self.use_adaptive:\n",
        "            # Process for adaptive model\n",
        "            processed_inputs = []\n",
        "            for data in receiver_data:\n",
        "                # 1D conv + maxpool simulation using numpy\n",
        "                # Downsample from 10001 to ~313 (factor of ~32)\n",
        "                downsampled = data[::32, :]  # (313, 31)\n",
        "\n",
        "                # Zero pad from 31 to 32 channels\n",
        "                if downsampled.shape[1] == 31:\n",
        "                    padded = np.pad(downsampled, ((0, 0), (0, 1)), mode='constant')  # (313, 32)\n",
        "                else:\n",
        "                    padded = downsampled\n",
        "\n",
        "                # Reshape to make it more compact 2D\n",
        "                # We'll treat this as (313, 32) for now and let the model handle it\n",
        "                processed_inputs.append(padded.T)  # (32, 313) for easier processing\n",
        "\n",
        "            # Stack all 5 processed inputs\n",
        "            input_tensor = np.stack(processed_inputs, axis=0)  # (5, 32, 313)\n",
        "        else:\n",
        "            # Process for resize model\n",
        "            processed_inputs = []\n",
        "            for data in receiver_data:\n",
        "                # Resize (10001, 31) to (300, 1259)\n",
        "                resized = ndimage.zoom(data, (300/10001, 1259/31), order=1)\n",
        "                processed_inputs.append(resized)\n",
        "\n",
        "            # Stack all 5 processed inputs\n",
        "            input_tensor = np.stack(processed_inputs, axis=0)  # (5, 300, 1259)\n",
        "\n",
        "        return torch.from_numpy(input_tensor), torch.from_numpy(target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyL109Cu5JAw"
      },
      "outputs": [],
      "source": [
        "# Helper function to match tensor sizes for skip connections\n",
        "def match_tensor_size(tensor1, tensor2):\n",
        "    \"\"\"Match the spatial dimensions of tensor1 to tensor2 by cropping or padding.\"\"\"\n",
        "    _, _, h1, w1 = tensor1.shape\n",
        "    _, _, h2, w2 = tensor2.shape\n",
        "\n",
        "    # Calculate differences\n",
        "    dh = h2 - h1\n",
        "    dw = w2 - w1\n",
        "\n",
        "    if dh > 0 or dw > 0:\n",
        "        # Pad tensor1 if it's smaller\n",
        "        pad_h = max(0, dh)\n",
        "        pad_w = max(0, dw)\n",
        "        tensor1 = F.pad(tensor1, (0, pad_w, 0, pad_h))\n",
        "    elif dh < 0 or dw < 0:\n",
        "        # Crop tensor1 if it's larger\n",
        "        tensor1 = tensor1[:, :, :h2, :w2]\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "# MAPE Loss Function\n",
        "def mape_loss(predictions, targets, epsilon=1e-8):\n",
        "    targets_safe = torch.clamp(torch.abs(targets), min=epsilon)\n",
        "    return torch.mean(torch.abs((targets - predictions) / targets_safe)) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoieEO_S5ASQ"
      },
      "outputs": [],
      "source": [
        "# Attention Block\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, dropout=0.05):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout2d(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "\n",
        "        query = self.conv1(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.conv2(x).view(batch_size, -1, width * height)\n",
        "        value = self.conv3(x).view(batch_size, -1, width * height)\n",
        "\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = self.softmax(attention)\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, channels, height, width)\n",
        "\n",
        "        return self.gamma * out + x\n",
        "\n",
        "# Double Convolution Block\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.05):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(dropout),  # Add dropout\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=5, out_channels=1, use_adaptive=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.use_adaptive = use_adaptive\n",
        "\n",
        "        if use_adaptive:\n",
        "            # First process the irregular input\n",
        "            self.input_processor = nn.Sequential(\n",
        "                nn.Conv2d(5, 16, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            # Input will be (5, 32, 313), output will be (32, 32, 313)\n",
        "            in_channels = 32\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = DoubleConv(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.enc4 = DoubleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck with attention\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "        self.attention = AttentionBlock(1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(1024, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64)\n",
        "\n",
        "        # Final output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "        if use_adaptive:\n",
        "            # Final upsampling to reach (300, 1259)\n",
        "            self.final_upsample = nn.Sequential(\n",
        "                nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_adaptive:\n",
        "            # Process irregular input first\n",
        "            x = self.input_processor(x)  # (batch, 32, 32, 313)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "        b = self.attention(b)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = self.up4(b)\n",
        "        d4 = match_tensor_size(d4, e4)\n",
        "        d4 = torch.cat([d4, e4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = match_tensor_size(d3, e3)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = match_tensor_size(d2, e2)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = match_tensor_size(d1, e1)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_conv(d1)\n",
        "\n",
        "        if self.use_adaptive:\n",
        "            # Upsample to final target size (300, 1259)\n",
        "            output = F.interpolate(output, size=(300, 1259), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC0Eyl_H5x38"
      },
      "outputs": [],
      "source": [
        "class StabilizedUNet(nn.Module):\n",
        "    def __init__(self, in_channels=5, out_channels=1, use_adaptive=True, dropout=0.05):\n",
        "        super(StabilizedUNet, self).__init__()\n",
        "        self.use_adaptive = use_adaptive\n",
        "\n",
        "        if use_adaptive:\n",
        "            self.input_processor = nn.Sequential(\n",
        "                nn.Conv2d(5, 16, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(16),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout2d(dropout),\n",
        "                nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            in_channels = 32\n",
        "\n",
        "        # Encoder with dropout\n",
        "        self.enc1 = DoubleConv(in_channels, 64, dropout)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(64, 128, dropout)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(128, 256, dropout)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.enc4 = DoubleConv(256, 512, dropout)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck with attention\n",
        "        self.bottleneck = DoubleConv(512, 1024, dropout)\n",
        "        self.attention = AttentionBlock(1024, dropout)\n",
        "\n",
        "        # Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(1024, 512, dropout)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(512, 256, dropout)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128, dropout)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64, dropout)\n",
        "\n",
        "        # Final output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "        # Initialize weights for stability\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Stable weight initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_adaptive:\n",
        "            x = self.input_processor(x)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "        b = self.attention(b)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = self.up4(b)\n",
        "        d4 = self._match_tensor_size(d4, e4)\n",
        "        d4 = torch.cat([d4, e4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = self._match_tensor_size(d3, e3)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = self._match_tensor_size(d2, e2)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = self._match_tensor_size(d1, e1)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        # Final output\n",
        "        output = self.final_conv(d1)\n",
        "\n",
        "        if self.use_adaptive:\n",
        "            output = F.interpolate(output, size=(300, 1259), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _match_tensor_size(self, tensor1, tensor2):\n",
        "        \"\"\"Helper function for matching tensor sizes\"\"\"\n",
        "        _, _, h1, w1 = tensor1.shape\n",
        "        _, _, h2, w2 = tensor2.shape\n",
        "\n",
        "        dh = h2 - h1\n",
        "        dw = w2 - w1\n",
        "\n",
        "        if dh > 0 or dw > 0:\n",
        "            pad_h = max(0, dh)\n",
        "            pad_w = max(0, dw)\n",
        "            tensor1 = F.pad(tensor1, (0, pad_w, 0, pad_h))\n",
        "        elif dh < 0 or dw < 0:\n",
        "            tensor1 = tensor1[:, :, :h2, :w2]\n",
        "\n",
        "        return tensor1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AGS5m7gWfZN"
      },
      "outputs": [],
      "source": [
        "def train_epoch_fast(model, train_loader, optimizer):\n",
        "    \"\"\"Fast training epoch without progress bars\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = mape_loss(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIP_NORM)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Minimal memory cleanup\n",
        "        if batch_idx % 20 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def validate_epoch_fast(model, val_loader):\n",
        "    \"\"\"Fast validation epoch without progress bars\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)\n",
        "            targets = targets.unsqueeze(1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = mape_loss(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Minimal memory cleanup\n",
        "            if batch_idx % 20 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def fast_train_single_fold(data_dir, fold_info, seed, result_dir):\n",
        "    \"\"\"Streamlined single fold training with specific seed\"\"\"\n",
        "\n",
        "    # AGGRESSIVE MEMORY CLEANUP AT START\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "\n",
        "    fold_num = fold_info['fold']\n",
        "\n",
        "    print(f\"🚀 Training Fold {fold_num} with Seed {seed}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Setup deterministic training\n",
        "    setup_deterministic_training(seed)\n",
        "\n",
        "    # Create model\n",
        "    model = StabilizedUNet(in_channels=5, out_channels=1, use_adaptive=USE_ADAPTIVE_MODEL,dropout=DROPOUT)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Optimized DataLoader settings for A100\n",
        "    train_dataset = SeismicDataset(data_dir, fold_info['train_indices'], use_adaptive=USE_ADAPTIVE_MODEL)\n",
        "    val_dataset = SeismicDataset(data_dir, fold_info['val_indices'], use_adaptive=USE_ADAPTIVE_MODEL)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,  # Optimized for stability\n",
        "        pin_memory=True,\n",
        "        persistent_workers=False,  # More stable\n",
        "        prefetch_factor=1\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=False,\n",
        "        prefetch_factor=1\n",
        "    )\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.005)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
        "\n",
        "    # Training tracking\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    epoch_times = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Train and validate\n",
        "        train_loss = train_epoch_fast(model, train_loader, optimizer)\n",
        "        val_loss = validate_epoch_fast(model, val_loader)\n",
        "\n",
        "        # Update scheduler\n",
        "        old_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Track metrics\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Progress reporting (minimal)\n",
        "        if epoch % 5 == 0 or val_loss < best_val_loss:\n",
        "            print(f\"  Epoch {epoch+1:2d}: Train={train_loss:.4f}%, Val={val_loss:.4f}%, LR={current_lr:.6f}, Time={epoch_time:.1f}s\")\n",
        "\n",
        "        # Learning rate change notification\n",
        "        if current_lr != old_lr:\n",
        "            print(f\"    📉 Learning rate reduced: {old_lr:.2e} → {current_lr:.2e}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save model checkpoint\n",
        "            model_path = result_dir / f'model_fold_{fold_num}_seed_{seed}.pth'\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': best_val_loss,\n",
        "                'fold': fold_num,\n",
        "                'seed': seed,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses\n",
        "            }, model_path)\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"    ⏹️  Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if current_lr < 1e-7:\n",
        "            print(f\"    ⏹️  Learning rate too low: {current_lr:.2e}\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"✅ Fold {fold_num} Seed {seed} complete: Best Val MAPE = {best_val_loss:.4f}% in {total_time/60:.1f}min\")\n",
        "\n",
        "    # Cleanup\n",
        "    del model, optimizer, scheduler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'fold': fold_num,\n",
        "        'seed': seed,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'final_epoch': epoch + 1,\n",
        "        'total_time': total_time,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'avg_epoch_time': np.mean(epoch_times)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vFX5LRxXVmV"
      },
      "outputs": [],
      "source": [
        "class OptimizedMultiSeedTrainer:\n",
        "    def __init__(self, data_dir, result_dir, k_folds=5, seeds=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.result_dir = Path(result_dir)\n",
        "        self.k_folds = k_folds\n",
        "        self.seeds = seeds or ENSEMBLE_SEEDS\n",
        "        self.results = {}\n",
        "\n",
        "        # Create results directory\n",
        "        self.result_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"🎯 Multi-Seed Trainer initialized\")\n",
        "        print(f\"   Data: {self.data_dir}\")\n",
        "        print(f\"   Results: {self.result_dir}\")\n",
        "        print(f\"   Seeds: {self.seeds}\")\n",
        "        print(f\"   Total models to train: {k_folds * len(self.seeds)}\")\n",
        "\n",
        "    def create_kfold_splits(self, dataset_size=2000):\n",
        "        \"\"\"Create K-fold splits (same as Phase 2b)\"\"\"\n",
        "        indices = np.arange(1, dataset_size + 1)\n",
        "        kfold = KFold(n_splits=self.k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        splits = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
        "            train_samples = indices[train_idx]\n",
        "            val_samples = indices[val_idx]\n",
        "\n",
        "            splits.append({\n",
        "                'fold': fold,\n",
        "                'train_indices': train_samples,\n",
        "                'val_indices': val_samples,\n",
        "                'train_size': len(train_samples),\n",
        "                'val_size': len(val_samples)\n",
        "            })\n",
        "\n",
        "        print(f\"📊 Created {self.k_folds}-fold splits\")\n",
        "        return splits\n",
        "\n",
        "    def train_fold_ensemble(self, fold_info):\n",
        "        \"\"\"Train multiple seeds for single fold\"\"\"\n",
        "        fold_num = fold_info['fold']\n",
        "        fold_results = []\n",
        "\n",
        "        print(f\"\\n🎯 Training Fold {fold_num} with {len(self.seeds)} seeds\")\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        for seed in self.seeds:\n",
        "            try:\n",
        "                result = fast_train_single_fold(self.data_dir, fold_info, seed, self.result_dir)\n",
        "                fold_results.append(result)\n",
        "\n",
        "                # Brief pause between models\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error training Fold {fold_num} Seed {seed}: {str(e)}\")\n",
        "                fold_results.append({\n",
        "                    'fold': fold_num,\n",
        "                    'seed': seed,\n",
        "                    'best_val_loss': float('inf'),\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "        fold_time = time.time() - fold_start_time\n",
        "\n",
        "        # Analyze fold ensemble\n",
        "        valid_results = [r for r in fold_results if 'error' not in r]\n",
        "        if valid_results:\n",
        "            performances = [r['best_val_loss'] for r in valid_results]\n",
        "            mean_perf = np.mean(performances)\n",
        "            std_perf = np.std(performances)\n",
        "            best_perf = np.min(performances)\n",
        "\n",
        "            print(f\"📈 Fold {fold_num} ensemble results:\")\n",
        "            print(f\"   Mean MAPE: {mean_perf:.4f}% ± {std_perf:.4f}%\")\n",
        "            print(f\"   Best MAPE: {best_perf:.4f}%\")\n",
        "            print(f\"   Seeds: {[r['seed'] for r in valid_results]}\")\n",
        "            print(f\"   Time: {fold_time/60:.1f} minutes\")\n",
        "\n",
        "        return fold_results\n",
        "\n",
        "    def run_full_training(self):\n",
        "        \"\"\"Run complete multi-seed ensemble training\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"🚀 STARTING OPTIMIZED MULTI-SEED ENSEMBLE TRAINING\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create fold splits\n",
        "        splits = self.create_kfold_splits()\n",
        "\n",
        "        # Train all folds\n",
        "        all_results = {}\n",
        "        for fold_info in splits:\n",
        "            fold_results = self.train_fold_ensemble(fold_info)\n",
        "            all_results[fold_info['fold']] = fold_results\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Comprehensive analysis\n",
        "        self.analyze_results(all_results, total_time)\n",
        "\n",
        "        # Save results\n",
        "        self.save_results(all_results)\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def analyze_results(self, all_results, total_time):\n",
        "        \"\"\"Comprehensive results analysis\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"📊 COMPREHENSIVE ENSEMBLE ANALYSIS\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Extract all valid results\n",
        "        all_performances = []\n",
        "        fold_best = []\n",
        "        fold_ensembles = []\n",
        "\n",
        "        print(f\"\\n📋 Individual Fold Analysis:\")\n",
        "        print(f\"{'Fold':<6} {'Best':<8} {'Mean':<8} {'Std':<8} {'Seeds':<12}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for fold_num in range(self.k_folds):\n",
        "            fold_results = all_results[fold_num]\n",
        "            valid_results = [r for r in fold_results if 'error' not in r]\n",
        "\n",
        "            if valid_results:\n",
        "                performances = [r['best_val_loss'] for r in valid_results]\n",
        "                all_performances.extend(performances)\n",
        "\n",
        "                fold_mean = np.mean(performances)\n",
        "                fold_std = np.std(performances)\n",
        "                fold_min = np.min(performances)\n",
        "\n",
        "                fold_best.append(fold_min)\n",
        "                fold_ensembles.append(fold_mean)\n",
        "\n",
        "                print(f\"{fold_num:<6} {fold_min:<8.4f} {fold_mean:<8.4f} {fold_std:<8.4f} {len(valid_results):<12}\")\n",
        "            else:\n",
        "                print(f\"{fold_num:<6} {'FAILED':<8} {'FAILED':<8} {'FAILED':<8} {'0':<12}\")\n",
        "                fold_best.append(float('inf'))\n",
        "                fold_ensembles.append(float('inf'))\n",
        "\n",
        "        # Overall statistics\n",
        "        if all_performances:\n",
        "            overall_mean = np.mean(all_performances)\n",
        "            overall_std = np.std(all_performances)\n",
        "            overall_best = np.min(all_performances)\n",
        "            ensemble_mean = np.mean(fold_ensembles)\n",
        "\n",
        "            print(f\"\\n🎯 Overall Performance Summary:\")\n",
        "            print(f\"   Total models trained: {len(all_performances)}\")\n",
        "            print(f\"   Overall mean MAPE: {overall_mean:.4f}% ± {overall_std:.4f}%\")\n",
        "            print(f\"   Best single model: {overall_best:.4f}%\")\n",
        "            print(f\"   Expected ensemble MAPE: {ensemble_mean:.4f}%\")\n",
        "            print(f\"   Total training time: {total_time/3600:.2f} hours\")\n",
        "            print(f\"   Average time per model: {total_time/len(all_performances)/60:.1f} minutes\")\n",
        "\n",
        "            # Performance improvements\n",
        "            best_fold_mean = np.mean(fold_best)\n",
        "            print(f\"\\n📈 Performance Analysis:\")\n",
        "            print(f\"   Best single fold performance: {np.min(fold_best):.4f}%\")\n",
        "            print(f\"   Mean of best fold performances: {best_fold_mean:.4f}%\")\n",
        "            print(f\"   Stability improvement: {overall_std:.4f}% standard deviation\")\n",
        "\n",
        "            # Competition readiness\n",
        "            target_performance = 3.0\n",
        "            models_below_target = sum(1 for p in all_performances if p < target_performance)\n",
        "            print(f\"\\n🏆 Competition Analysis:\")\n",
        "            print(f\"   Models below 3.0% target: {models_below_target}/{len(all_performances)}\")\n",
        "            print(f\"   Success rate: {models_below_target/len(all_performances)*100:.1f}%\")\n",
        "\n",
        "            if overall_best < target_performance:\n",
        "                print(f\"   🎉 TARGET ACHIEVED! Best model: {overall_best:.4f}%\")\n",
        "            else:\n",
        "                gap = overall_best - target_performance\n",
        "                print(f\"   🎯 Gap to target: {gap:.4f}%\")\n",
        "\n",
        "    def save_results(self, all_results):\n",
        "        \"\"\"Save comprehensive results\"\"\"\n",
        "        import json\n",
        "\n",
        "        # Save detailed results\n",
        "        results_file = self.result_dir / 'ensemble_results.json'\n",
        "\n",
        "        # Convert to JSON-serializable format\n",
        "        json_results = {}\n",
        "        for fold_num, fold_results in all_results.items():\n",
        "            json_results[f'fold_{fold_num}'] = []\n",
        "            for result in fold_results:\n",
        "                json_result = {k: v for k, v in result.items() if k not in ['train_losses', 'val_losses']}\n",
        "                json_results[f'fold_{fold_num}'].append(json_result)\n",
        "\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "        # Create summary file\n",
        "        summary_file = self.result_dir / 'training_summary.txt'\n",
        "        with open(summary_file, 'w') as f:\n",
        "            f.write(\"Phase 2c Multi-Seed Ensemble Training Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            for fold_num in range(self.k_folds):\n",
        "                fold_results = all_results[fold_num]\n",
        "                valid_results = [r for r in fold_results if 'error' not in r]\n",
        "\n",
        "                if valid_results:\n",
        "                    performances = [r['best_val_loss'] for r in valid_results]\n",
        "                    f.write(f\"Fold {fold_num}:\\n\")\n",
        "                    f.write(f\"  Best: {np.min(performances):.4f}%\\n\")\n",
        "                    f.write(f\"  Mean: {np.mean(performances):.4f}% ± {np.std(performances):.4f}%\\n\")\n",
        "                    f.write(f\"  Seeds: {[r['seed'] for r in valid_results]}\\n\\n\")\n",
        "\n",
        "        print(f\"💾 Results saved to:\")\n",
        "        print(f\"   {results_file}\")\n",
        "        print(f\"   {summary_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Q-iexRXpMr"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    # Set memory management environment variable\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "    # Setup paths\n",
        "\n",
        "    data_dir = Path('/content/drive/MyDrive/ThinkOnward/Data/Train')\n",
        "    result_dir = Path('/content/drive/MyDrive/ThinkOnward/Result/Phase2c')\n",
        "\n",
        "    print(f\"🔧 Phase 2c Optimized Training Pipeline\")\n",
        "    print(f\"   Target: Sub-3.0% MAPE with stable ensemble\")\n",
        "    print(f\"   Strategy: {K_FOLDS} folds × {len(ENSEMBLE_SEEDS)} seeds = {K_FOLDS * len(ENSEMBLE_SEEDS)} models\")\n",
        "    print(f\"   Hardware: A100 GPU with batch_size={BATCH_SIZE}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = OptimizedMultiSeedTrainer(\n",
        "        data_dir=data_dir,\n",
        "        result_dir=result_dir,\n",
        "        k_folds=K_FOLDS,\n",
        "        seeds=ENSEMBLE_SEEDS\n",
        "    )\n",
        "\n",
        "    # Run training\n",
        "    results = trainer.run_full_training()\n",
        "\n",
        "    print(f\"\\n✅ Phase 2c Training Complete!\")\n",
        "    print(f\"Ready for diffusion model development in Phase 3\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KKucxFXXXme",
        "collapsed": true,
        "outputId": "4b7c1a34-3005-4735-e235-99fa5b969cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Phase 2c Optimized Training Pipeline\n",
            "   Target: Sub-3.0% MAPE with stable ensemble\n",
            "   Strategy: 5 folds × 3 seeds = 15 models\n",
            "   Hardware: A100 GPU with batch_size=32\n",
            "🎯 Multi-Seed Trainer initialized\n",
            "   Data: /content/drive/MyDrive/ThinkOnward/Data/Train\n",
            "   Results: /content/drive/MyDrive/ThinkOnward/Result/Phase2c\n",
            "   Seeds: [42, 123, 456]\n",
            "   Total models to train: 15\n",
            "\n",
            "============================================================\n",
            "🚀 STARTING OPTIMIZED MULTI-SEED ENSEMBLE TRAINING\n",
            "============================================================\n",
            "📊 Created 5-fold splits\n",
            "\n",
            "🎯 Training Fold 0 with 3 seeds\n",
            "🚀 Training Fold 0 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-5cad78f6f013>:18: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  attention = torch.bmm(query, key)\n",
            "<ipython-input-6-5cad78f6f013>:21: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  out = torch.bmm(value, attention.permute(0, 2, 1))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Epoch  1: Train=77.3063%, Val=95.8216%, LR=0.000050, Time=1680.4s\n",
            "  Epoch  2: Train=57.2865%, Val=54.2104%, LR=0.000050, Time=34.5s\n",
            "  Epoch  3: Train=50.6860%, Val=48.6783%, LR=0.000050, Time=41.7s\n",
            "  Epoch  5: Train=45.3088%, Val=48.0459%, LR=0.000050, Time=33.0s\n",
            "  Epoch  6: Train=43.3685%, Val=43.2842%, LR=0.000050, Time=41.8s\n",
            "  Epoch  7: Train=41.6620%, Val=41.8366%, LR=0.000050, Time=41.7s\n",
            "  Epoch  9: Train=38.3403%, Val=39.6044%, LR=0.000050, Time=32.6s\n",
            "  Epoch 11: Train=35.1564%, Val=29.5073%, LR=0.000050, Time=33.1s\n",
            "  Epoch 16: Train=28.5392%, Val=38.6225%, LR=0.000050, Time=33.0s\n",
            "    📉 Learning rate reduced: 5.00e-05 → 2.50e-05\n",
            "  Epoch 19: Train=25.5515%, Val=29.3514%, LR=0.000025, Time=33.0s\n",
            "  Epoch 21: Train=24.1497%, Val=24.8773%, LR=0.000025, Time=33.1s\n",
            "  Epoch 26: Train=21.2963%, Val=24.2036%, LR=0.000025, Time=33.7s\n",
            "✅ Fold 0 Seed 42 complete: Best Val MAPE = 24.2036% in 45.7min\n",
            "🚀 Training Fold 0 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=86.4768%, Val=100.8492%, LR=0.000050, Time=33.1s\n",
            "  Epoch  2: Train=70.9706%, Val=84.2635%, LR=0.000050, Time=34.2s\n",
            "  Epoch  3: Train=63.5841%, Val=63.2556%, LR=0.000050, Time=42.3s\n",
            "  Epoch  4: Train=58.7907%, Val=61.1778%, LR=0.000050, Time=42.2s\n",
            "  Epoch  5: Train=55.8156%, Val=59.1872%, LR=0.000050, Time=41.8s\n",
            "  Epoch  6: Train=53.4014%, Val=54.1154%, LR=0.000050, Time=42.2s\n",
            "  Epoch  8: Train=49.4387%, Val=50.2404%, LR=0.000050, Time=33.1s\n",
            "  Epoch  9: Train=47.7307%, Val=44.3797%, LR=0.000050, Time=42.0s\n",
            "  Epoch 11: Train=44.4940%, Val=48.6576%, LR=0.000050, Time=33.4s\n",
            "  Epoch 12: Train=42.8740%, Val=40.6776%, LR=0.000050, Time=34.0s\n",
            "  Epoch 16: Train=36.7913%, Val=43.2344%, LR=0.000050, Time=33.4s\n",
            "  Epoch 17: Train=35.3306%, Val=39.2807%, LR=0.000050, Time=33.3s\n",
            "  Epoch 21: Train=29.8100%, Val=44.0120%, LR=0.000050, Time=33.9s\n",
            "  Epoch 23: Train=27.0822%, Val=31.9099%, LR=0.000050, Time=33.4s\n",
            "  Epoch 25: Train=24.2245%, Val=28.5073%, LR=0.000050, Time=33.0s\n",
            "  Epoch 26: Train=22.6551%, Val=27.2852%, LR=0.000050, Time=42.3s\n",
            "  Epoch 28: Train=19.7385%, Val=26.7615%, LR=0.000050, Time=33.1s\n",
            "✅ Fold 0 Seed 123 complete: Best Val MAPE = 26.7615% in 18.8min\n",
            "🚀 Training Fold 0 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=74.7937%, Val=95.8180%, LR=0.000050, Time=33.0s\n",
            "  Epoch  2: Train=59.7373%, Val=57.6253%, LR=0.000050, Time=34.4s\n",
            "  Epoch  3: Train=53.4028%, Val=51.9477%, LR=0.000050, Time=42.1s\n",
            "  Epoch  4: Train=49.2429%, Val=48.0961%, LR=0.000050, Time=41.6s\n",
            "  Epoch  5: Train=46.2564%, Val=45.8759%, LR=0.000050, Time=41.6s\n",
            "  Epoch  6: Train=44.0288%, Val=44.5556%, LR=0.000050, Time=42.4s\n",
            "  Epoch  7: Train=42.1218%, Val=43.0721%, LR=0.000050, Time=42.5s\n",
            "  Epoch 10: Train=37.3151%, Val=37.4727%, LR=0.000050, Time=32.7s\n",
            "  Epoch 11: Train=35.5883%, Val=41.5690%, LR=0.000050, Time=41.7s\n",
            "  Epoch 14: Train=31.2394%, Val=37.3139%, LR=0.000050, Time=32.9s\n",
            "  Epoch 16: Train=28.5676%, Val=39.9960%, LR=0.000050, Time=32.9s\n",
            "  Epoch 17: Train=27.3910%, Val=33.1519%, LR=0.000050, Time=32.9s\n",
            "  Epoch 18: Train=26.3190%, Val=29.3159%, LR=0.000050, Time=41.8s\n",
            "  Epoch 21: Train=22.9539%, Val=37.3852%, LR=0.000050, Time=33.3s\n",
            "  Epoch 23: Train=20.6842%, Val=23.6903%, LR=0.000050, Time=33.0s\n",
            "  Epoch 25: Train=18.4749%, Val=21.9990%, LR=0.000050, Time=33.1s\n",
            "  Epoch 26: Train=17.4713%, Val=36.5006%, LR=0.000050, Time=41.5s\n",
            "  Epoch 28: Train=15.3371%, Val=16.3603%, LR=0.000050, Time=33.2s\n",
            "✅ Fold 0 Seed 456 complete: Best Val MAPE = 16.3603% in 18.7min\n",
            "📈 Fold 0 ensemble results:\n",
            "   Mean MAPE: 22.4418% ± 4.4253%\n",
            "   Best MAPE: 16.3603%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 83.3 minutes\n",
            "\n",
            "🎯 Training Fold 1 with 3 seeds\n",
            "🚀 Training Fold 1 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=77.4947%, Val=95.1139%, LR=0.000050, Time=33.7s\n",
            "  Epoch  2: Train=57.3930%, Val=54.0129%, LR=0.000050, Time=35.0s\n",
            "  Epoch  3: Train=50.7044%, Val=48.8871%, LR=0.000050, Time=42.3s\n",
            "  Epoch  4: Train=47.6373%, Val=46.5788%, LR=0.000050, Time=42.1s\n",
            "  Epoch  5: Train=45.3264%, Val=44.7250%, LR=0.000050, Time=41.8s\n",
            "  Epoch  6: Train=43.3883%, Val=46.3476%, LR=0.000050, Time=42.3s\n",
            "  Epoch  7: Train=41.7715%, Val=42.4220%, LR=0.000050, Time=33.1s\n",
            "  Epoch  9: Train=38.7916%, Val=39.9528%, LR=0.000050, Time=33.6s\n",
            "  Epoch 10: Train=37.3432%, Val=37.9562%, LR=0.000050, Time=42.4s\n",
            "  Epoch 11: Train=35.9423%, Val=42.0362%, LR=0.000050, Time=42.9s\n",
            "  Epoch 16: Train=29.4375%, Val=32.5228%, LR=0.000050, Time=33.0s\n",
            "  Epoch 21: Train=23.1901%, Val=37.6262%, LR=0.000050, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.00e-05 → 2.50e-05\n",
            "  Epoch 23: Train=20.8800%, Val=30.6929%, LR=0.000025, Time=33.4s\n",
            "  Epoch 26: Train=19.1971%, Val=32.5099%, LR=0.000025, Time=33.7s\n",
            "  Epoch 27: Train=18.6412%, Val=28.7219%, LR=0.000025, Time=33.2s\n",
            "  Epoch 28: Train=18.1592%, Val=20.7915%, LR=0.000025, Time=42.2s\n",
            "  Epoch 30: Train=17.0528%, Val=20.0989%, LR=0.000025, Time=33.4s\n",
            "✅ Fold 1 Seed 42 complete: Best Val MAPE = 20.0989% in 18.6min\n",
            "🚀 Training Fold 1 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=86.4823%, Val=100.4386%, LR=0.000050, Time=40.6s\n",
            "  Epoch  2: Train=70.9115%, Val=84.4625%, LR=0.000050, Time=32.9s\n",
            "  Epoch  3: Train=63.5893%, Val=63.1656%, LR=0.000050, Time=41.5s\n",
            "  Epoch  4: Train=58.8001%, Val=51.7413%, LR=0.000050, Time=42.2s\n",
            "  Epoch  6: Train=53.6230%, Val=54.7369%, LR=0.000050, Time=33.3s\n",
            "  Epoch  8: Train=49.7594%, Val=50.8052%, LR=0.000050, Time=32.8s\n",
            "  Epoch  9: Train=48.1745%, Val=48.9804%, LR=0.000050, Time=41.9s\n",
            "  Epoch 10: Train=46.3747%, Val=46.0516%, LR=0.000050, Time=42.1s\n",
            "  Epoch 11: Train=44.6802%, Val=49.2741%, LR=0.000050, Time=41.8s\n",
            "  Epoch 12: Train=43.0515%, Val=37.0691%, LR=0.000050, Time=32.8s\n",
            "  Epoch 13: Train=41.5403%, Val=35.4482%, LR=0.000050, Time=41.3s\n",
            "  Epoch 16: Train=36.9163%, Val=45.1000%, LR=0.000050, Time=33.5s\n",
            "    📉 Learning rate reduced: 5.00e-05 → 2.50e-05\n",
            "  Epoch 21: Train=30.5224%, Val=43.1797%, LR=0.000025, Time=32.7s\n",
            "    ⏹️  Early stopping at epoch 21\n",
            "✅ Fold 1 Seed 123 complete: Best Val MAPE = 35.4482% in 13.0min\n",
            "🚀 Training Fold 1 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=74.7777%, Val=96.2535%, LR=0.000050, Time=33.1s\n",
            "  Epoch  2: Train=59.6983%, Val=59.7111%, LR=0.000050, Time=34.0s\n",
            "  Epoch  3: Train=53.2445%, Val=51.8935%, LR=0.000050, Time=41.9s\n",
            "  Epoch  4: Train=49.0354%, Val=48.4762%, LR=0.000050, Time=42.1s\n",
            "  Epoch  5: Train=46.0099%, Val=46.0525%, LR=0.000050, Time=41.4s\n",
            "  Epoch  6: Train=43.7543%, Val=45.2078%, LR=0.000050, Time=42.3s\n",
            "  Epoch  7: Train=41.8539%, Val=43.3285%, LR=0.000050, Time=42.3s\n",
            "  Epoch  8: Train=40.0996%, Val=41.6207%, LR=0.000050, Time=41.8s\n",
            "  Epoch 10: Train=36.9265%, Val=38.6794%, LR=0.000050, Time=33.0s\n",
            "  Epoch 11: Train=35.5540%, Val=41.8786%, LR=0.000050, Time=42.2s\n",
            "  Epoch 14: Train=31.5869%, Val=30.7287%, LR=0.000050, Time=33.1s\n",
            "  Epoch 16: Train=29.0172%, Val=36.8133%, LR=0.000050, Time=33.0s\n",
            "  Epoch 18: Train=26.5204%, Val=29.0453%, LR=0.000050, Time=33.3s\n",
            "  Epoch 21: Train=22.7476%, Val=37.9293%, LR=0.000050, Time=33.7s\n",
            "  Epoch 22: Train=21.5620%, Val=26.2442%, LR=0.000050, Time=33.4s\n",
            "  Epoch 25: Train=18.1394%, Val=22.9015%, LR=0.000050, Time=33.6s\n",
            "  Epoch 26: Train=16.9131%, Val=21.0055%, LR=0.000050, Time=42.2s\n",
            "  Epoch 27: Train=15.7346%, Val=20.7394%, LR=0.000050, Time=42.5s\n",
            "  Epoch 28: Train=14.6239%, Val=19.6279%, LR=0.000050, Time=42.1s\n",
            "  Epoch 29: Train=13.6144%, Val=19.5369%, LR=0.000050, Time=41.9s\n",
            "✅ Fold 1 Seed 456 complete: Best Val MAPE = 19.5369% in 19.3min\n",
            "📈 Fold 1 ensemble results:\n",
            "   Mean MAPE: 25.0280% ± 7.3718%\n",
            "   Best MAPE: 19.5369%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 51.0 minutes\n",
            "\n",
            "🎯 Training Fold 2 with 3 seeds\n",
            "🚀 Training Fold 2 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=77.4318%, Val=96.4046%, LR=0.000050, Time=33.1s\n",
            "  Epoch  2: Train=57.4093%, Val=50.0413%, LR=0.000050, Time=34.5s\n",
            "  Epoch  4: Train=47.5453%, Val=46.6006%, LR=0.000050, Time=33.3s\n",
            "  Epoch  5: Train=45.2809%, Val=44.0668%, LR=0.000050, Time=41.6s\n",
            "  Epoch  6: Train=43.4891%, Val=42.4074%, LR=0.000050, Time=41.3s\n",
            "  Epoch  7: Train=41.9428%, Val=40.7060%, LR=0.000050, Time=41.1s\n",
            "  Epoch  9: Train=38.7333%, Val=40.4929%, LR=0.000050, Time=32.8s\n",
            "  Epoch 10: Train=37.3527%, Val=39.8439%, LR=0.000050, Time=41.7s\n",
            "  Epoch 11: Train=36.0283%, Val=40.3004%, LR=0.000050, Time=41.8s\n",
            "  Epoch 13: Train=33.5197%, Val=39.5158%, LR=0.000050, Time=33.9s\n",
            "  Epoch 14: Train=32.1421%, Val=39.3884%, LR=0.000050, Time=41.5s\n",
            "  Epoch 15: Train=30.9291%, Val=37.6497%, LR=0.000050, Time=41.6s\n",
            "  Epoch 16: Train=29.7094%, Val=39.3082%, LR=0.000050, Time=41.6s\n",
            "  Epoch 17: Train=28.4529%, Val=36.6095%, LR=0.000050, Time=32.7s\n",
            "  Epoch 18: Train=27.2142%, Val=31.3472%, LR=0.000050, Time=41.7s\n",
            "  Epoch 21: Train=23.7439%, Val=37.8195%, LR=0.000050, Time=32.9s\n",
            "  Epoch 22: Train=22.7201%, Val=30.7314%, LR=0.000050, Time=32.6s\n",
            "  Epoch 23: Train=21.7274%, Val=29.0073%, LR=0.000050, Time=41.4s\n",
            "  Epoch 26: Train=19.0496%, Val=33.3088%, LR=0.000050, Time=32.9s\n",
            "  Epoch 28: Train=17.2892%, Val=25.6562%, LR=0.000050, Time=33.1s\n",
            "  Epoch 29: Train=16.2662%, Val=23.1365%, LR=0.000050, Time=41.7s\n",
            "  Epoch 30: Train=15.3673%, Val=22.0234%, LR=0.000050, Time=41.5s\n",
            "✅ Fold 2 Seed 42 complete: Best Val MAPE = 22.0234% in 19.1min\n",
            "🚀 Training Fold 2 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=86.4683%, Val=100.3882%, LR=0.000050, Time=41.4s\n",
            "  Epoch  2: Train=70.9350%, Val=83.4071%, LR=0.000050, Time=34.9s\n",
            "  Epoch  3: Train=63.2465%, Val=62.3845%, LR=0.000050, Time=42.3s\n",
            "  Epoch  4: Train=58.6135%, Val=58.7728%, LR=0.000050, Time=42.0s\n",
            "  Epoch  5: Train=55.8105%, Val=54.4553%, LR=0.000050, Time=41.9s\n",
            "  Epoch  6: Train=53.3882%, Val=52.6027%, LR=0.000050, Time=42.8s\n",
            "  Epoch  7: Train=51.3813%, Val=51.1553%, LR=0.000050, Time=41.9s\n",
            "  Epoch  8: Train=49.4983%, Val=49.5366%, LR=0.000050, Time=41.8s\n",
            "  Epoch  9: Train=47.7645%, Val=46.3312%, LR=0.000050, Time=34.3s\n",
            "  Epoch 11: Train=44.3689%, Val=42.1741%, LR=0.000050, Time=37.3s\n",
            "  Epoch 15: Train=38.2339%, Val=41.3959%, LR=0.000050, Time=33.2s\n",
            "  Epoch 16: Train=36.7876%, Val=43.3278%, LR=0.000050, Time=41.7s\n",
            "  Epoch 17: Train=35.3156%, Val=39.6549%, LR=0.000050, Time=33.3s\n",
            "  Epoch 18: Train=33.8280%, Val=39.0301%, LR=0.000050, Time=42.8s\n",
            "  Epoch 19: Train=32.3740%, Val=35.9834%, LR=0.000050, Time=42.3s\n",
            "  Epoch 20: Train=31.0524%, Val=34.9702%, LR=0.000050, Time=42.1s\n",
            "  Epoch 21: Train=29.6793%, Val=30.8875%, LR=0.000050, Time=42.5s\n",
            "  Epoch 26: Train=22.0275%, Val=38.1091%, LR=0.000050, Time=32.9s\n",
            "  Epoch 27: Train=20.3191%, Val=26.5233%, LR=0.000050, Time=33.4s\n",
            "  Epoch 29: Train=17.2206%, Val=23.9240%, LR=0.000050, Time=33.7s\n",
            "✅ Fold 2 Seed 123 complete: Best Val MAPE = 23.9240% in 19.4min\n",
            "🚀 Training Fold 2 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=74.7630%, Val=95.8200%, LR=0.000050, Time=33.8s\n",
            "  Epoch  2: Train=59.7019%, Val=60.8209%, LR=0.000050, Time=33.4s\n",
            "  Epoch  3: Train=53.3355%, Val=52.2368%, LR=0.000050, Time=41.4s\n",
            "  Epoch  4: Train=49.2493%, Val=48.1396%, LR=0.000050, Time=42.1s\n",
            "  Epoch  5: Train=46.2563%, Val=45.5411%, LR=0.000050, Time=42.0s\n",
            "  Epoch  6: Train=43.9886%, Val=45.2150%, LR=0.000050, Time=42.1s\n",
            "  Epoch  7: Train=42.1192%, Val=44.6513%, LR=0.000050, Time=42.4s\n",
            "  Epoch  8: Train=40.4221%, Val=42.8347%, LR=0.000050, Time=41.8s\n",
            "  Epoch  9: Train=38.8261%, Val=37.2941%, LR=0.000050, Time=41.6s\n",
            "  Epoch 11: Train=35.7737%, Val=42.8849%, LR=0.000050, Time=32.8s\n",
            "  Epoch 14: Train=31.5825%, Val=33.6259%, LR=0.000050, Time=32.7s\n",
            "  Epoch 16: Train=29.1811%, Val=39.8891%, LR=0.000050, Time=33.1s\n",
            "  Epoch 17: Train=27.9530%, Val=30.1902%, LR=0.000050, Time=33.1s\n",
            "  Epoch 18: Train=26.7432%, Val=30.0864%, LR=0.000050, Time=41.7s\n",
            "  Epoch 19: Train=25.5172%, Val=29.4024%, LR=0.000050, Time=41.6s\n",
            "  Epoch 20: Train=24.3596%, Val=28.0513%, LR=0.000050, Time=41.7s\n",
            "  Epoch 21: Train=23.3854%, Val=33.9689%, LR=0.000050, Time=41.8s\n",
            "  Epoch 23: Train=20.8480%, Val=27.1917%, LR=0.000050, Time=32.8s\n",
            "  Epoch 24: Train=19.5915%, Val=22.4171%, LR=0.000050, Time=42.2s\n",
            "  Epoch 26: Train=16.6810%, Val=17.9556%, LR=0.000050, Time=32.8s\n",
            "  Epoch 30: Train=10.5388%, Val=17.0856%, LR=0.000050, Time=33.5s\n",
            "✅ Fold 2 Seed 456 complete: Best Val MAPE = 17.0856% in 19.2min\n",
            "📈 Fold 2 ensemble results:\n",
            "   Mean MAPE: 21.0110% ± 2.8821%\n",
            "   Best MAPE: 17.0856%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 57.8 minutes\n",
            "\n",
            "🎯 Training Fold 3 with 3 seeds\n",
            "🚀 Training Fold 3 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=77.3320%, Val=96.9520%, LR=0.000050, Time=41.2s\n",
            "  Epoch  2: Train=57.3864%, Val=62.1875%, LR=0.000050, Time=34.7s\n",
            "  Epoch  3: Train=50.7806%, Val=49.1152%, LR=0.000050, Time=41.3s\n",
            "  Epoch  4: Train=47.5875%, Val=47.0394%, LR=0.000050, Time=41.6s\n",
            "  Epoch  5: Train=45.3412%, Val=44.0347%, LR=0.000050, Time=42.0s\n",
            "  Epoch  6: Train=43.3216%, Val=42.6390%, LR=0.000050, Time=41.6s\n",
            "  Epoch  7: Train=41.6756%, Val=42.3550%, LR=0.000050, Time=41.6s\n",
            "  Epoch  8: Train=40.1117%, Val=41.6823%, LR=0.000050, Time=41.6s\n",
            "  Epoch 10: Train=37.2654%, Val=40.2058%, LR=0.000050, Time=33.2s\n",
            "  Epoch 11: Train=36.0653%, Val=39.6623%, LR=0.000050, Time=41.8s\n",
            "  Epoch 13: Train=33.5752%, Val=33.4297%, LR=0.000050, Time=33.2s\n",
            "  Epoch 16: Train=29.8472%, Val=39.2977%, LR=0.000050, Time=33.1s\n",
            "  Epoch 19: Train=26.2126%, Val=31.9688%, LR=0.000050, Time=32.9s\n",
            "  Epoch 21: Train=24.0089%, Val=33.4923%, LR=0.000050, Time=32.7s\n",
            "  Epoch 22: Train=22.8087%, Val=31.3839%, LR=0.000050, Time=32.7s\n",
            "  Epoch 24: Train=20.6678%, Val=28.1876%, LR=0.000050, Time=32.9s\n",
            "  Epoch 26: Train=18.7353%, Val=28.6673%, LR=0.000050, Time=33.0s\n",
            "  Epoch 27: Train=17.7093%, Val=24.0431%, LR=0.000050, Time=32.9s\n",
            "  Epoch 28: Train=16.7799%, Val=20.7074%, LR=0.000050, Time=41.4s\n",
            "✅ Fold 3 Seed 42 complete: Best Val MAPE = 20.7074% in 19.1min\n",
            "🚀 Training Fold 3 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=86.4658%, Val=101.1321%, LR=0.000050, Time=32.6s\n",
            "  Epoch  2: Train=70.9488%, Val=82.8853%, LR=0.000050, Time=34.6s\n",
            "  Epoch  3: Train=63.5576%, Val=61.5420%, LR=0.000050, Time=41.8s\n",
            "  Epoch  4: Train=58.7890%, Val=58.4002%, LR=0.000050, Time=41.8s\n",
            "  Epoch  5: Train=55.8479%, Val=55.3164%, LR=0.000050, Time=42.0s\n",
            "  Epoch  6: Train=53.5494%, Val=57.0519%, LR=0.000050, Time=40.9s\n",
            "  Epoch  8: Train=49.7303%, Val=48.6440%, LR=0.000050, Time=32.6s\n",
            "  Epoch  9: Train=48.0501%, Val=48.3377%, LR=0.000050, Time=41.1s\n",
            "  Epoch 10: Train=46.4339%, Val=41.9342%, LR=0.000050, Time=41.2s\n",
            "  Epoch 11: Train=44.8304%, Val=43.5654%, LR=0.000050, Time=41.3s\n",
            "  Epoch 13: Train=41.7794%, Val=39.3694%, LR=0.000050, Time=33.1s\n",
            "  Epoch 15: Train=38.6632%, Val=30.6933%, LR=0.000050, Time=32.8s\n",
            "  Epoch 16: Train=37.3020%, Val=40.3281%, LR=0.000050, Time=41.4s\n",
            "  Epoch 21: Train=29.9022%, Val=31.9778%, LR=0.000025, Time=32.4s\n",
            "    📉 Learning rate reduced: 5.00e-05 → 2.50e-05\n",
            "    ⏹️  Early stopping at epoch 23\n",
            "✅ Fold 3 Seed 123 complete: Best Val MAPE = 30.6933% in 14.1min\n",
            "🚀 Training Fold 3 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=74.7820%, Val=95.1648%, LR=0.000050, Time=32.8s\n",
            "  Epoch  2: Train=59.6660%, Val=57.4404%, LR=0.000050, Time=32.9s\n",
            "  Epoch  3: Train=53.3456%, Val=50.1547%, LR=0.000050, Time=41.8s\n",
            "  Epoch  4: Train=49.2979%, Val=46.6725%, LR=0.000050, Time=41.2s\n",
            "  Epoch  5: Train=46.2376%, Val=46.3959%, LR=0.000050, Time=41.8s\n",
            "  Epoch  6: Train=43.9096%, Val=42.5527%, LR=0.000050, Time=41.5s\n",
            "  Epoch  9: Train=38.6608%, Val=42.3545%, LR=0.000050, Time=33.0s\n",
            "  Epoch 10: Train=37.1283%, Val=41.6484%, LR=0.000050, Time=41.6s\n",
            "  Epoch 11: Train=35.6180%, Val=40.4343%, LR=0.000050, Time=41.9s\n",
            "  Epoch 13: Train=32.8427%, Val=40.2217%, LR=0.000050, Time=33.2s\n",
            "  Epoch 14: Train=31.4886%, Val=30.8736%, LR=0.000050, Time=41.7s\n",
            "  Epoch 16: Train=28.7894%, Val=37.5433%, LR=0.000050, Time=33.4s\n",
            "  Epoch 18: Train=26.4234%, Val=29.2237%, LR=0.000050, Time=32.6s\n",
            "  Epoch 21: Train=22.4319%, Val=37.9888%, LR=0.000050, Time=32.7s\n",
            "  Epoch 22: Train=21.1488%, Val=23.3166%, LR=0.000050, Time=32.7s\n",
            "  Epoch 26: Train=16.6490%, Val=34.5409%, LR=0.000050, Time=32.6s\n",
            "  Epoch 27: Train=15.4350%, Val=21.0406%, LR=0.000050, Time=32.5s\n",
            "  Epoch 28: Train=14.2489%, Val=18.3141%, LR=0.000050, Time=42.0s\n",
            "  Epoch 30: Train=11.9106%, Val=17.5785%, LR=0.000050, Time=32.0s\n",
            "✅ Fold 3 Seed 456 complete: Best Val MAPE = 17.5785% in 18.7min\n",
            "📈 Fold 3 ensemble results:\n",
            "   Mean MAPE: 22.9930% ± 5.5927%\n",
            "   Best MAPE: 17.5785%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 52.0 minutes\n",
            "\n",
            "🎯 Training Fold 4 with 3 seeds\n",
            "🚀 Training Fold 4 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=77.4268%, Val=95.8948%, LR=0.000050, Time=41.0s\n",
            "  Epoch  2: Train=57.4740%, Val=54.2705%, LR=0.000050, Time=33.3s\n",
            "  Epoch  3: Train=50.6213%, Val=48.1241%, LR=0.000050, Time=41.8s\n",
            "  Epoch  4: Train=47.5297%, Val=46.0064%, LR=0.000050, Time=41.8s\n",
            "  Epoch  6: Train=43.4119%, Val=42.9962%, LR=0.000050, Time=32.4s\n",
            "  Epoch  7: Train=41.7982%, Val=42.7092%, LR=0.000050, Time=41.7s\n",
            "  Epoch  8: Train=40.2158%, Val=42.5807%, LR=0.000050, Time=41.5s\n",
            "  Epoch  9: Train=38.8007%, Val=42.1308%, LR=0.000050, Time=41.7s\n",
            "  Epoch 10: Train=37.2095%, Val=40.7575%, LR=0.000050, Time=41.8s\n",
            "  Epoch 11: Train=35.8809%, Val=40.0420%, LR=0.000050, Time=41.7s\n",
            "  Epoch 13: Train=33.2453%, Val=39.3645%, LR=0.000050, Time=32.8s\n",
            "  Epoch 14: Train=31.9769%, Val=39.3645%, LR=0.000050, Time=41.3s\n",
            "  Epoch 16: Train=29.3216%, Val=39.1503%, LR=0.000050, Time=32.9s\n",
            "  Epoch 17: Train=28.1106%, Val=38.6965%, LR=0.000050, Time=41.8s\n",
            "  Epoch 18: Train=26.9309%, Val=37.1189%, LR=0.000050, Time=41.9s\n",
            "  Epoch 19: Train=25.7328%, Val=37.0884%, LR=0.000050, Time=41.8s\n",
            "  Epoch 20: Train=24.5487%, Val=36.2658%, LR=0.000050, Time=41.2s\n",
            "  Epoch 21: Train=23.4715%, Val=35.4761%, LR=0.000050, Time=41.5s\n",
            "  Epoch 22: Train=22.3707%, Val=31.6704%, LR=0.000050, Time=41.6s\n",
            "  Epoch 26: Train=18.0470%, Val=33.3529%, LR=0.000050, Time=32.8s\n",
            "  Epoch 28: Train=16.2143%, Val=25.5122%, LR=0.000050, Time=32.4s\n",
            "  Epoch 30: Train=14.3940%, Val=14.1874%, LR=0.000050, Time=32.7s\n",
            "✅ Fold 4 Seed 42 complete: Best Val MAPE = 14.1874% in 19.7min\n",
            "🚀 Training Fold 4 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=86.5121%, Val=100.3992%, LR=0.000050, Time=41.1s\n",
            "  Epoch  2: Train=70.8488%, Val=82.0801%, LR=0.000050, Time=32.8s\n",
            "  Epoch  3: Train=63.2819%, Val=62.1014%, LR=0.000050, Time=41.8s\n",
            "  Epoch  4: Train=58.7399%, Val=59.7611%, LR=0.000050, Time=41.7s\n",
            "  Epoch  5: Train=55.8793%, Val=54.1104%, LR=0.000050, Time=42.1s\n",
            "  Epoch  6: Train=53.5836%, Val=55.1217%, LR=0.000050, Time=42.0s\n",
            "  Epoch  7: Train=51.5715%, Val=53.9258%, LR=0.000050, Time=32.7s\n",
            "  Epoch  8: Train=49.7631%, Val=52.3458%, LR=0.000050, Time=41.6s\n",
            "  Epoch  9: Train=48.0817%, Val=51.7647%, LR=0.000050, Time=41.6s\n",
            "  Epoch 10: Train=46.4835%, Val=50.1712%, LR=0.000050, Time=41.7s\n",
            "  Epoch 11: Train=44.9503%, Val=44.4888%, LR=0.000050, Time=41.9s\n",
            "  Epoch 12: Train=43.3084%, Val=41.4460%, LR=0.000050, Time=41.9s\n",
            "  Epoch 13: Train=41.6392%, Val=40.9281%, LR=0.000050, Time=41.5s\n",
            "  Epoch 15: Train=38.7252%, Val=39.9906%, LR=0.000050, Time=32.5s\n",
            "  Epoch 16: Train=37.2852%, Val=42.3827%, LR=0.000050, Time=41.1s\n",
            "  Epoch 18: Train=34.2320%, Val=37.3303%, LR=0.000050, Time=32.6s\n",
            "  Epoch 21: Train=29.8749%, Val=38.7104%, LR=0.000050, Time=32.8s\n",
            "  Epoch 22: Train=28.3254%, Val=35.5710%, LR=0.000050, Time=32.6s\n",
            "  Epoch 26: Train=22.0306%, Val=28.0966%, LR=0.000050, Time=33.1s\n",
            "  Epoch 29: Train=17.1196%, Val=23.0023%, LR=0.000050, Time=32.3s\n",
            "✅ Fold 4 Seed 123 complete: Best Val MAPE = 23.0023% in 19.2min\n",
            "🚀 Training Fold 4 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=74.7939%, Val=95.9650%, LR=0.000050, Time=32.9s\n",
            "  Epoch  2: Train=59.7549%, Val=65.5569%, LR=0.000050, Time=34.0s\n",
            "  Epoch  3: Train=53.2804%, Val=51.4861%, LR=0.000050, Time=42.1s\n",
            "  Epoch  4: Train=49.1877%, Val=47.6007%, LR=0.000050, Time=41.7s\n",
            "  Epoch  5: Train=46.1275%, Val=45.9482%, LR=0.000050, Time=41.0s\n",
            "  Epoch  6: Train=43.8193%, Val=43.3585%, LR=0.000050, Time=41.5s\n",
            "  Epoch  8: Train=40.2298%, Val=43.1222%, LR=0.000050, Time=33.0s\n",
            "  Epoch  9: Train=38.5558%, Val=41.7999%, LR=0.000050, Time=41.2s\n",
            "  Epoch 10: Train=36.9985%, Val=41.2804%, LR=0.000050, Time=41.6s\n",
            "  Epoch 11: Train=35.4996%, Val=39.3298%, LR=0.000050, Time=41.7s\n",
            "  Epoch 12: Train=34.1423%, Val=36.3315%, LR=0.000050, Time=41.8s\n",
            "  Epoch 16: Train=28.8452%, Val=30.8080%, LR=0.000050, Time=32.8s\n",
            "  Epoch 18: Train=26.3933%, Val=30.1027%, LR=0.000050, Time=32.5s\n",
            "  Epoch 21: Train=22.6611%, Val=24.5131%, LR=0.000050, Time=32.4s\n",
            "  Epoch 25: Train=17.7622%, Val=23.3903%, LR=0.000050, Time=33.0s\n",
            "  Epoch 26: Train=16.6323%, Val=37.3615%, LR=0.000050, Time=41.3s\n",
            "  Epoch 28: Train=14.2589%, Val=19.8182%, LR=0.000050, Time=32.6s\n",
            "✅ Fold 4 Seed 456 complete: Best Val MAPE = 19.8182% in 18.9min\n",
            "📈 Fold 4 ensemble results:\n",
            "   Mean MAPE: 19.0027% ± 3.6446%\n",
            "   Best MAPE: 14.1874%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 57.8 minutes\n",
            "\n",
            "============================================================\n",
            "📊 COMPREHENSIVE ENSEMBLE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "📋 Individual Fold Analysis:\n",
            "Fold   Best     Mean     Std      Seeds       \n",
            "--------------------------------------------------\n",
            "0      16.3603  22.4418  4.4253   3           \n",
            "1      19.5369  25.0280  7.3718   3           \n",
            "2      17.0856  21.0110  2.8821   3           \n",
            "3      17.5785  22.9930  5.5927   3           \n",
            "4      14.1874  19.0027  3.6446   3           \n",
            "\n",
            "🎯 Overall Performance Summary:\n",
            "   Total models trained: 15\n",
            "   Overall mean MAPE: 22.0953% ± 5.4233%\n",
            "   Best single model: 14.1874%\n",
            "   Expected ensemble MAPE: 22.0953%\n",
            "   Total training time: 5.03 hours\n",
            "   Average time per model: 20.1 minutes\n",
            "\n",
            "📈 Performance Analysis:\n",
            "   Best single fold performance: 14.1874%\n",
            "   Mean of best fold performances: 16.9497%\n",
            "   Stability improvement: 5.4233% standard deviation\n",
            "\n",
            "🏆 Competition Analysis:\n",
            "   Models below 3.0% target: 0/15\n",
            "   Success rate: 0.0%\n",
            "   🎯 Gap to target: 11.1874%\n",
            "💾 Results saved to:\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/ensemble_results.json\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/training_summary.txt\n",
            "\n",
            "✅ Phase 2c Training Complete!\n",
            "Ready for diffusion model development in Phase 3\n"
          ]
        }
      ],
      "source": [
        "# Run the optimized training for original Unet\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the optimized training for StabilizedUnet\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vafjc7WqztxY",
        "outputId": "a4e43665-9caa-45be-9367-d6a1f2141059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Phase 2c Optimized Training Pipeline\n",
            "   Target: Sub-3.0% MAPE with stable ensemble\n",
            "   Strategy: 5 folds × 3 seeds = 15 models\n",
            "   Hardware: A100 GPU with batch_size=32\n",
            "🎯 Multi-Seed Trainer initialized\n",
            "   Data: /content/drive/MyDrive/ThinkOnward/Data/Train\n",
            "   Results: /content/drive/MyDrive/ThinkOnward/Result/Phase2c\n",
            "   Seeds: [42, 123, 456]\n",
            "   Total models to train: 15\n",
            "\n",
            "============================================================\n",
            "🚀 STARTING OPTIMIZED MULTI-SEED ENSEMBLE TRAINING\n",
            "============================================================\n",
            "📊 Created 5-fold splits\n",
            "\n",
            "🎯 Training Fold 0 with 3 seeds\n",
            "🚀 Training Fold 0 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-46e5a01bdf9d>:19: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  attention = torch.bmm(query, key)\n",
            "<ipython-input-15-46e5a01bdf9d>:22: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  out = torch.bmm(value, attention.permute(0, 2, 1))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  1: Train=136.7605%, Val=38.4531%, LR=0.000050, Time=5513.2s\n",
            "  Epoch  2: Train=41.3112%, Val=20.9334%, LR=0.000050, Time=36.0s\n",
            "  Epoch  3: Train=23.9276%, Val=12.8215%, LR=0.000050, Time=43.9s\n",
            "  Epoch  4: Train=18.3436%, Val=9.6654%, LR=0.000050, Time=44.7s\n",
            "  Epoch  5: Train=15.4221%, Val=8.7806%, LR=0.000050, Time=43.6s\n",
            "  Epoch  6: Train=13.5812%, Val=7.9705%, LR=0.000050, Time=43.6s\n",
            "  Epoch  7: Train=12.3909%, Val=7.6224%, LR=0.000050, Time=44.9s\n",
            "  Epoch  8: Train=11.5377%, Val=6.9995%, LR=0.000050, Time=44.1s\n",
            "  Epoch  9: Train=10.8307%, Val=6.8610%, LR=0.000050, Time=44.4s\n",
            "  Epoch 10: Train=10.1804%, Val=6.3582%, LR=0.000050, Time=43.9s\n",
            "  Epoch 11: Train=9.8002%, Val=6.1080%, LR=0.000050, Time=44.4s\n",
            "  Epoch 12: Train=9.3976%, Val=5.6898%, LR=0.000050, Time=44.7s\n",
            "  Epoch 13: Train=8.8665%, Val=5.5334%, LR=0.000050, Time=44.2s\n",
            "  Epoch 14: Train=8.4973%, Val=5.4660%, LR=0.000050, Time=44.5s\n",
            "  Epoch 16: Train=8.0429%, Val=4.9705%, LR=0.000050, Time=34.9s\n",
            "  Epoch 18: Train=7.5495%, Val=4.9169%, LR=0.000050, Time=34.8s\n",
            "  Epoch 19: Train=7.3331%, Val=4.7393%, LR=0.000050, Time=42.9s\n",
            "  Epoch 21: Train=7.1474%, Val=4.7796%, LR=0.000050, Time=33.6s\n",
            "  Epoch 22: Train=6.9130%, Val=4.7251%, LR=0.000050, Time=34.0s\n",
            "  Epoch 23: Train=6.8218%, Val=4.7062%, LR=0.000050, Time=43.4s\n",
            "  Epoch 24: Train=6.7167%, Val=4.5826%, LR=0.000050, Time=44.0s\n",
            "  Epoch 26: Train=6.4576%, Val=4.3623%, LR=0.000050, Time=35.3s\n",
            "  Epoch 29: Train=6.2017%, Val=4.2638%, LR=0.000050, Time=36.0s\n",
            "✅ Fold 0 Seed 42 complete: Best Val MAPE = 4.2638% in 112.4min\n",
            "🚀 Training Fold 0 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=94.9706%, Val=34.6421%, LR=0.000050, Time=34.8s\n",
            "  Epoch  2: Train=37.8804%, Val=16.8341%, LR=0.000050, Time=36.0s\n",
            "  Epoch  3: Train=24.3809%, Val=13.2016%, LR=0.000050, Time=43.4s\n",
            "  Epoch  4: Train=18.6181%, Val=11.6827%, LR=0.000050, Time=43.3s\n",
            "  Epoch  5: Train=15.6834%, Val=9.9341%, LR=0.000050, Time=43.1s\n",
            "  Epoch  6: Train=13.8176%, Val=9.0770%, LR=0.000050, Time=43.6s\n",
            "  Epoch  7: Train=12.6241%, Val=8.5679%, LR=0.000050, Time=43.5s\n",
            "  Epoch  8: Train=11.7064%, Val=7.4580%, LR=0.000050, Time=43.5s\n",
            "  Epoch  9: Train=11.2051%, Val=7.3378%, LR=0.000050, Time=44.0s\n",
            "  Epoch 10: Train=10.6519%, Val=7.2659%, LR=0.000050, Time=43.3s\n",
            "  Epoch 11: Train=10.0384%, Val=7.2320%, LR=0.000050, Time=43.7s\n",
            "  Epoch 12: Train=9.4242%, Val=6.7715%, LR=0.000050, Time=44.3s\n",
            "  Epoch 13: Train=8.9680%, Val=6.1526%, LR=0.000050, Time=44.4s\n",
            "  Epoch 16: Train=8.1394%, Val=6.0524%, LR=0.000050, Time=34.8s\n",
            "  Epoch 18: Train=7.6701%, Val=5.5759%, LR=0.000050, Time=34.6s\n",
            "  Epoch 21: Train=7.1342%, Val=5.4120%, LR=0.000050, Time=34.4s\n",
            "  Epoch 26: Train=6.7330%, Val=4.9911%, LR=0.000050, Time=34.2s\n",
            "  Epoch 28: Train=6.4847%, Val=4.7760%, LR=0.000050, Time=35.2s\n",
            "✅ Fold 0 Seed 123 complete: Best Val MAPE = 4.7760% in 20.2min\n",
            "🚀 Training Fold 0 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=117.1529%, Val=33.1309%, LR=0.000050, Time=35.2s\n",
            "  Epoch  2: Train=42.3382%, Val=18.9348%, LR=0.000050, Time=36.6s\n",
            "  Epoch  3: Train=25.1562%, Val=12.1165%, LR=0.000050, Time=44.4s\n",
            "  Epoch  4: Train=18.7955%, Val=10.3983%, LR=0.000050, Time=43.9s\n",
            "  Epoch  5: Train=15.7553%, Val=8.6544%, LR=0.000050, Time=43.8s\n",
            "  Epoch  6: Train=13.9331%, Val=8.6387%, LR=0.000050, Time=44.4s\n",
            "  Epoch  7: Train=12.9251%, Val=7.5223%, LR=0.000050, Time=44.1s\n",
            "  Epoch  9: Train=11.2267%, Val=6.8181%, LR=0.000050, Time=35.1s\n",
            "  Epoch 10: Train=10.5887%, Val=6.2890%, LR=0.000050, Time=44.0s\n",
            "  Epoch 11: Train=10.1078%, Val=6.4419%, LR=0.000050, Time=44.0s\n",
            "  Epoch 13: Train=9.2823%, Val=6.0588%, LR=0.000050, Time=35.4s\n",
            "  Epoch 14: Train=8.7628%, Val=5.4144%, LR=0.000050, Time=44.5s\n",
            "  Epoch 16: Train=8.1603%, Val=5.3623%, LR=0.000050, Time=35.8s\n",
            "  Epoch 18: Train=7.7015%, Val=4.8230%, LR=0.000050, Time=35.3s\n",
            "  Epoch 21: Train=7.2212%, Val=4.6496%, LR=0.000050, Time=34.9s\n",
            "  Epoch 22: Train=7.1075%, Val=4.6478%, LR=0.000050, Time=44.6s\n",
            "  Epoch 24: Train=6.8394%, Val=4.5597%, LR=0.000050, Time=35.4s\n",
            "  Epoch 25: Train=6.7081%, Val=4.5343%, LR=0.000050, Time=44.1s\n",
            "  Epoch 26: Train=6.7748%, Val=5.4794%, LR=0.000050, Time=44.3s\n",
            "  Epoch 27: Train=6.6120%, Val=4.5206%, LR=0.000050, Time=34.4s\n",
            "  Epoch 28: Train=6.5126%, Val=4.4640%, LR=0.000050, Time=42.8s\n",
            "  Epoch 29: Train=6.3136%, Val=4.3590%, LR=0.000050, Time=42.9s\n",
            "  Epoch 30: Train=6.2264%, Val=4.2916%, LR=0.000050, Time=43.5s\n",
            "✅ Fold 0 Seed 456 complete: Best Val MAPE = 4.2916% in 20.8min\n",
            "📈 Fold 0 ensemble results:\n",
            "   Mean MAPE: 4.4438% ± 0.2352%\n",
            "   Best MAPE: 4.2638%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 153.4 minutes\n",
            "\n",
            "🎯 Training Fold 1 with 3 seeds\n",
            "🚀 Training Fold 1 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=136.8001%, Val=38.7599%, LR=0.000050, Time=43.9s\n",
            "  Epoch  2: Train=41.2641%, Val=20.0442%, LR=0.000050, Time=36.5s\n",
            "  Epoch  3: Train=23.5929%, Val=13.2470%, LR=0.000050, Time=44.1s\n",
            "  Epoch  4: Train=17.9279%, Val=8.9595%, LR=0.000050, Time=43.3s\n",
            "  Epoch  5: Train=15.3356%, Val=8.5139%, LR=0.000050, Time=44.1s\n",
            "  Epoch  6: Train=13.4816%, Val=7.5116%, LR=0.000050, Time=44.6s\n",
            "  Epoch  8: Train=11.4483%, Val=6.6552%, LR=0.000050, Time=35.2s\n",
            "  Epoch  9: Train=10.7308%, Val=6.3847%, LR=0.000050, Time=44.1s\n",
            "  Epoch 10: Train=10.2595%, Val=6.0797%, LR=0.000050, Time=44.4s\n",
            "  Epoch 11: Train=9.6414%, Val=5.8021%, LR=0.000050, Time=44.1s\n",
            "  Epoch 12: Train=9.1807%, Val=5.5219%, LR=0.000050, Time=44.2s\n",
            "  Epoch 15: Train=8.2904%, Val=5.0378%, LR=0.000050, Time=35.0s\n",
            "  Epoch 16: Train=7.9610%, Val=4.8871%, LR=0.000050, Time=44.0s\n",
            "  Epoch 21: Train=7.0905%, Val=4.9339%, LR=0.000050, Time=35.0s\n",
            "  Epoch 22: Train=6.9223%, Val=4.3905%, LR=0.000050, Time=34.8s\n",
            "  Epoch 26: Train=6.4962%, Val=4.3967%, LR=0.000050, Time=34.8s\n",
            "  Epoch 27: Train=6.3816%, Val=4.3119%, LR=0.000050, Time=34.9s\n",
            "  Epoch 29: Train=6.3129%, Val=4.2209%, LR=0.000050, Time=34.8s\n",
            "✅ Fold 1 Seed 42 complete: Best Val MAPE = 4.2209% in 20.2min\n",
            "🚀 Training Fold 1 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=94.9427%, Val=35.1734%, LR=0.000050, Time=35.4s\n",
            "  Epoch  2: Train=38.5318%, Val=17.3632%, LR=0.000050, Time=36.9s\n",
            "  Epoch  3: Train=24.6811%, Val=12.3036%, LR=0.000050, Time=44.0s\n",
            "  Epoch  4: Train=18.6411%, Val=11.2797%, LR=0.000050, Time=44.5s\n",
            "  Epoch  5: Train=15.5084%, Val=10.3217%, LR=0.000050, Time=44.3s\n",
            "  Epoch  6: Train=13.9489%, Val=8.5953%, LR=0.000050, Time=44.4s\n",
            "  Epoch  7: Train=12.6860%, Val=7.5893%, LR=0.000050, Time=43.5s\n",
            "  Epoch  9: Train=11.2452%, Val=7.0432%, LR=0.000050, Time=33.7s\n",
            "  Epoch 11: Train=10.1026%, Val=7.0581%, LR=0.000050, Time=34.5s\n",
            "  Epoch 12: Train=9.4163%, Val=6.0622%, LR=0.000050, Time=33.8s\n",
            "  Epoch 13: Train=9.0299%, Val=5.9274%, LR=0.000050, Time=44.6s\n",
            "  Epoch 14: Train=8.7041%, Val=5.9267%, LR=0.000050, Time=44.3s\n",
            "  Epoch 15: Train=8.3285%, Val=5.6541%, LR=0.000050, Time=44.2s\n",
            "  Epoch 16: Train=8.1478%, Val=5.4668%, LR=0.000050, Time=44.4s\n",
            "  Epoch 20: Train=7.3379%, Val=5.2901%, LR=0.000050, Time=34.0s\n",
            "  Epoch 21: Train=7.2152%, Val=5.0603%, LR=0.000050, Time=42.9s\n",
            "  Epoch 26: Train=6.7402%, Val=4.6761%, LR=0.000050, Time=33.8s\n",
            "✅ Fold 1 Seed 123 complete: Best Val MAPE = 4.6761% in 19.7min\n",
            "🚀 Training Fold 1 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=117.1931%, Val=33.4798%, LR=0.000050, Time=33.8s\n",
            "  Epoch  2: Train=42.3424%, Val=19.4971%, LR=0.000050, Time=34.6s\n",
            "  Epoch  3: Train=25.1429%, Val=12.6722%, LR=0.000050, Time=42.5s\n",
            "  Epoch  4: Train=18.9029%, Val=10.6844%, LR=0.000050, Time=42.4s\n",
            "  Epoch  5: Train=15.8162%, Val=8.9873%, LR=0.000050, Time=42.5s\n",
            "  Epoch  6: Train=13.8338%, Val=7.7815%, LR=0.000050, Time=42.2s\n",
            "  Epoch  7: Train=12.6298%, Val=7.0583%, LR=0.000050, Time=42.3s\n",
            "  Epoch  9: Train=11.1925%, Val=6.6564%, LR=0.000050, Time=34.2s\n",
            "  Epoch 10: Train=10.6320%, Val=6.3244%, LR=0.000050, Time=42.4s\n",
            "  Epoch 11: Train=10.1429%, Val=6.1388%, LR=0.000050, Time=43.2s\n",
            "  Epoch 12: Train=9.6567%, Val=5.9261%, LR=0.000050, Time=42.3s\n",
            "  Epoch 14: Train=8.8356%, Val=5.2753%, LR=0.000050, Time=33.5s\n",
            "  Epoch 16: Train=8.3106%, Val=4.9551%, LR=0.000050, Time=34.1s\n",
            "  Epoch 18: Train=7.8181%, Val=4.8223%, LR=0.000050, Time=34.6s\n",
            "  Epoch 19: Train=7.6448%, Val=4.7549%, LR=0.000050, Time=42.5s\n",
            "  Epoch 21: Train=7.2736%, Val=4.5872%, LR=0.000050, Time=34.1s\n",
            "  Epoch 26: Train=6.8651%, Val=4.9572%, LR=0.000050, Time=33.6s\n",
            "  Epoch 27: Train=6.5360%, Val=4.3789%, LR=0.000050, Time=33.3s\n",
            "  Epoch 29: Train=6.3684%, Val=4.3207%, LR=0.000050, Time=33.7s\n",
            "  Epoch 30: Train=6.1972%, Val=4.2291%, LR=0.000050, Time=42.8s\n",
            "✅ Fold 1 Seed 456 complete: Best Val MAPE = 4.2291% in 19.7min\n",
            "📈 Fold 1 ensemble results:\n",
            "   Mean MAPE: 4.3754% ± 0.2127%\n",
            "   Best MAPE: 4.2209%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 59.8 minutes\n",
            "\n",
            "🎯 Training Fold 2 with 3 seeds\n",
            "🚀 Training Fold 2 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=136.8271%, Val=39.0487%, LR=0.000050, Time=42.0s\n",
            "  Epoch  2: Train=41.6393%, Val=20.0385%, LR=0.000050, Time=35.6s\n",
            "  Epoch  3: Train=24.2989%, Val=14.1339%, LR=0.000050, Time=42.6s\n",
            "  Epoch  4: Train=18.1703%, Val=9.7336%, LR=0.000050, Time=42.3s\n",
            "  Epoch  5: Train=15.0508%, Val=8.8540%, LR=0.000050, Time=42.6s\n",
            "  Epoch  6: Train=13.1492%, Val=7.3687%, LR=0.000050, Time=42.8s\n",
            "  Epoch  8: Train=11.4012%, Val=7.2679%, LR=0.000050, Time=34.1s\n",
            "  Epoch  9: Train=10.7602%, Val=6.8381%, LR=0.000050, Time=42.5s\n",
            "  Epoch 10: Train=10.1955%, Val=6.3975%, LR=0.000050, Time=42.5s\n",
            "  Epoch 11: Train=9.5428%, Val=6.0630%, LR=0.000050, Time=42.3s\n",
            "  Epoch 12: Train=9.0915%, Val=5.5484%, LR=0.000050, Time=42.4s\n",
            "  Epoch 13: Train=8.6271%, Val=5.5390%, LR=0.000050, Time=42.2s\n",
            "  Epoch 14: Train=8.3366%, Val=5.2920%, LR=0.000050, Time=42.7s\n",
            "  Epoch 15: Train=8.0369%, Val=5.0887%, LR=0.000050, Time=42.9s\n",
            "  Epoch 16: Train=7.8071%, Val=5.0213%, LR=0.000050, Time=43.0s\n",
            "  Epoch 17: Train=7.7100%, Val=5.0118%, LR=0.000050, Time=42.6s\n",
            "  Epoch 18: Train=7.4937%, Val=4.9043%, LR=0.000050, Time=42.5s\n",
            "  Epoch 20: Train=7.0705%, Val=4.8180%, LR=0.000050, Time=33.8s\n",
            "  Epoch 21: Train=7.0178%, Val=4.9030%, LR=0.000050, Time=42.2s\n",
            "  Epoch 22: Train=6.7580%, Val=4.4854%, LR=0.000050, Time=33.4s\n",
            "  Epoch 26: Train=6.4120%, Val=4.3165%, LR=0.000050, Time=33.7s\n",
            "  Epoch 29: Train=6.2702%, Val=4.2856%, LR=0.000050, Time=33.5s\n",
            "✅ Fold 2 Seed 42 complete: Best Val MAPE = 4.2856% in 20.3min\n",
            "🚀 Training Fold 2 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=94.8129%, Val=35.0609%, LR=0.000050, Time=33.3s\n",
            "  Epoch  2: Train=38.2734%, Val=17.3488%, LR=0.000050, Time=32.9s\n",
            "  Epoch  3: Train=24.8408%, Val=13.4450%, LR=0.000050, Time=41.8s\n",
            "  Epoch  4: Train=18.6219%, Val=10.8849%, LR=0.000050, Time=42.0s\n",
            "  Epoch  5: Train=15.6244%, Val=10.0681%, LR=0.000050, Time=42.1s\n",
            "  Epoch  6: Train=13.7466%, Val=8.6201%, LR=0.000050, Time=41.9s\n",
            "  Epoch  7: Train=12.5324%, Val=7.7380%, LR=0.000050, Time=42.2s\n",
            "  Epoch  9: Train=11.0458%, Val=7.1756%, LR=0.000050, Time=33.5s\n",
            "  Epoch 10: Train=10.4560%, Val=7.1214%, LR=0.000050, Time=42.5s\n",
            "  Epoch 11: Train=9.8157%, Val=7.2286%, LR=0.000050, Time=42.1s\n",
            "  Epoch 12: Train=9.2967%, Val=6.0814%, LR=0.000050, Time=32.8s\n",
            "  Epoch 15: Train=8.2757%, Val=5.9133%, LR=0.000050, Time=33.7s\n",
            "  Epoch 16: Train=8.0080%, Val=5.3538%, LR=0.000050, Time=42.2s\n",
            "  Epoch 20: Train=7.3013%, Val=5.1110%, LR=0.000050, Time=34.0s\n",
            "  Epoch 21: Train=7.0913%, Val=5.0237%, LR=0.000050, Time=42.1s\n",
            "  Epoch 26: Train=6.6290%, Val=5.0021%, LR=0.000050, Time=33.6s\n",
            "  Epoch 27: Train=6.4397%, Val=4.6757%, LR=0.000050, Time=42.0s\n",
            "  Epoch 28: Train=6.4807%, Val=4.4337%, LR=0.000050, Time=41.9s\n",
            "✅ Fold 2 Seed 123 complete: Best Val MAPE = 4.4337% in 19.3min\n",
            "🚀 Training Fold 2 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=117.1945%, Val=33.4586%, LR=0.000050, Time=32.9s\n",
            "  Epoch  2: Train=42.3038%, Val=18.7524%, LR=0.000050, Time=34.5s\n",
            "  Epoch  3: Train=25.1501%, Val=14.3893%, LR=0.000050, Time=42.0s\n",
            "  Epoch  4: Train=19.1094%, Val=10.6259%, LR=0.000050, Time=41.9s\n",
            "  Epoch  5: Train=15.7719%, Val=9.1614%, LR=0.000050, Time=42.3s\n",
            "  Epoch  6: Train=13.8040%, Val=8.1392%, LR=0.000050, Time=42.2s\n",
            "  Epoch  7: Train=12.7302%, Val=7.2959%, LR=0.000050, Time=42.6s\n",
            "  Epoch  9: Train=11.0280%, Val=6.6820%, LR=0.000050, Time=34.5s\n",
            "  Epoch 10: Train=10.5203%, Val=6.3138%, LR=0.000050, Time=42.5s\n",
            "  Epoch 11: Train=10.0137%, Val=6.2011%, LR=0.000050, Time=42.2s\n",
            "  Epoch 13: Train=9.2847%, Val=5.8276%, LR=0.000050, Time=33.6s\n",
            "  Epoch 14: Train=8.6949%, Val=5.4200%, LR=0.000050, Time=42.8s\n",
            "  Epoch 16: Train=8.0751%, Val=5.0529%, LR=0.000050, Time=33.3s\n",
            "  Epoch 18: Train=7.6853%, Val=4.8403%, LR=0.000050, Time=33.2s\n",
            "  Epoch 21: Train=7.2406%, Val=4.6283%, LR=0.000050, Time=33.7s\n",
            "  Epoch 23: Train=6.9024%, Val=4.5918%, LR=0.000050, Time=33.9s\n",
            "  Epoch 25: Train=6.6538%, Val=4.4252%, LR=0.000050, Time=33.6s\n",
            "  Epoch 26: Train=6.7147%, Val=4.8215%, LR=0.000050, Time=42.5s\n",
            "  Epoch 29: Train=6.4951%, Val=4.3163%, LR=0.000050, Time=33.4s\n",
            "✅ Fold 2 Seed 456 complete: Best Val MAPE = 4.3163% in 19.6min\n",
            "📈 Fold 2 ensemble results:\n",
            "   Mean MAPE: 4.3452% ± 0.0638%\n",
            "   Best MAPE: 4.2856%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 59.3 minutes\n",
            "\n",
            "🎯 Training Fold 3 with 3 seeds\n",
            "🚀 Training Fold 3 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=136.6250%, Val=38.9199%, LR=0.000050, Time=34.0s\n",
            "  Epoch  2: Train=40.9699%, Val=19.6726%, LR=0.000050, Time=34.9s\n",
            "  Epoch  3: Train=23.5678%, Val=13.5415%, LR=0.000050, Time=42.4s\n",
            "  Epoch  4: Train=17.8754%, Val=11.2863%, LR=0.000050, Time=42.7s\n",
            "  Epoch  5: Train=14.9092%, Val=8.9343%, LR=0.000050, Time=42.9s\n",
            "  Epoch  6: Train=13.1347%, Val=8.3445%, LR=0.000050, Time=42.9s\n",
            "  Epoch  7: Train=12.0178%, Val=7.4826%, LR=0.000050, Time=42.3s\n",
            "  Epoch  8: Train=11.3125%, Val=7.3786%, LR=0.000050, Time=43.0s\n",
            "  Epoch  9: Train=10.7508%, Val=6.6845%, LR=0.000050, Time=42.7s\n",
            "  Epoch 10: Train=10.1017%, Val=6.1993%, LR=0.000050, Time=42.7s\n",
            "  Epoch 11: Train=9.5742%, Val=6.2236%, LR=0.000050, Time=42.7s\n",
            "  Epoch 12: Train=9.2768%, Val=5.7852%, LR=0.000050, Time=33.5s\n",
            "  Epoch 13: Train=8.7463%, Val=5.4939%, LR=0.000050, Time=42.2s\n",
            "  Epoch 14: Train=8.3832%, Val=5.4885%, LR=0.000050, Time=42.8s\n",
            "  Epoch 15: Train=8.0842%, Val=5.3069%, LR=0.000050, Time=42.5s\n",
            "  Epoch 16: Train=7.9791%, Val=5.1160%, LR=0.000050, Time=43.0s\n",
            "  Epoch 18: Train=7.4454%, Val=4.9531%, LR=0.000050, Time=33.5s\n",
            "  Epoch 19: Train=7.3553%, Val=4.7665%, LR=0.000050, Time=43.2s\n",
            "  Epoch 20: Train=7.0772%, Val=4.7629%, LR=0.000050, Time=42.3s\n",
            "  Epoch 21: Train=7.1222%, Val=4.7559%, LR=0.000050, Time=43.3s\n",
            "  Epoch 22: Train=6.9052%, Val=4.6813%, LR=0.000050, Time=43.6s\n",
            "  Epoch 24: Train=6.6388%, Val=4.6451%, LR=0.000050, Time=33.5s\n",
            "  Epoch 25: Train=6.4737%, Val=4.3539%, LR=0.000050, Time=43.0s\n",
            "  Epoch 26: Train=6.3867%, Val=4.3646%, LR=0.000050, Time=43.4s\n",
            "  Epoch 28: Train=6.1501%, Val=4.3407%, LR=0.000050, Time=33.8s\n",
            "✅ Fold 3 Seed 42 complete: Best Val MAPE = 4.3407% in 20.6min\n",
            "🚀 Training Fold 3 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=94.8935%, Val=35.4057%, LR=0.000050, Time=34.2s\n",
            "  Epoch  2: Train=38.4768%, Val=16.7670%, LR=0.000050, Time=35.6s\n",
            "  Epoch  3: Train=24.6686%, Val=13.7960%, LR=0.000050, Time=43.3s\n",
            "  Epoch  4: Train=18.9225%, Val=12.0642%, LR=0.000050, Time=43.1s\n",
            "  Epoch  5: Train=15.9267%, Val=10.4250%, LR=0.000050, Time=43.5s\n",
            "  Epoch  6: Train=14.0610%, Val=9.2982%, LR=0.000050, Time=43.1s\n",
            "  Epoch  7: Train=12.6733%, Val=8.8877%, LR=0.000050, Time=42.9s\n",
            "  Epoch  8: Train=11.7149%, Val=7.8998%, LR=0.000050, Time=43.1s\n",
            "  Epoch  9: Train=10.9977%, Val=7.7419%, LR=0.000050, Time=43.3s\n",
            "  Epoch 11: Train=9.7929%, Val=7.2664%, LR=0.000050, Time=34.5s\n",
            "  Epoch 12: Train=9.1409%, Val=6.0705%, LR=0.000050, Time=43.4s\n",
            "  Epoch 16: Train=7.9568%, Val=6.0280%, LR=0.000050, Time=34.9s\n",
            "  Epoch 18: Train=7.6130%, Val=5.3419%, LR=0.000050, Time=34.2s\n",
            "  Epoch 20: Train=7.2913%, Val=5.0008%, LR=0.000050, Time=34.5s\n",
            "  Epoch 21: Train=7.0794%, Val=5.1583%, LR=0.000050, Time=42.8s\n",
            "  Epoch 26: Train=6.6697%, Val=4.9793%, LR=0.000050, Time=34.1s\n",
            "  Epoch 27: Train=6.3607%, Val=4.9094%, LR=0.000050, Time=43.3s\n",
            "  Epoch 28: Train=6.4392%, Val=4.8069%, LR=0.000050, Time=43.0s\n",
            "  Epoch 29: Train=6.3337%, Val=4.6381%, LR=0.000050, Time=43.1s\n",
            "✅ Fold 3 Seed 123 complete: Best Val MAPE = 4.6381% in 20.0min\n",
            "🚀 Training Fold 3 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=117.2136%, Val=33.5895%, LR=0.000050, Time=34.3s\n",
            "  Epoch  2: Train=41.8438%, Val=18.2647%, LR=0.000050, Time=35.9s\n",
            "  Epoch  3: Train=24.9892%, Val=14.6997%, LR=0.000050, Time=43.3s\n",
            "  Epoch  4: Train=18.9430%, Val=11.0477%, LR=0.000050, Time=43.4s\n",
            "  Epoch  5: Train=15.7333%, Val=8.9736%, LR=0.000050, Time=43.0s\n",
            "  Epoch  6: Train=13.8521%, Val=8.1352%, LR=0.000050, Time=42.7s\n",
            "  Epoch  7: Train=12.6131%, Val=7.3486%, LR=0.000050, Time=43.5s\n",
            "  Epoch  9: Train=11.0863%, Val=6.9460%, LR=0.000050, Time=34.1s\n",
            "  Epoch 10: Train=10.5022%, Val=6.7768%, LR=0.000050, Time=43.6s\n",
            "  Epoch 11: Train=10.1536%, Val=6.2028%, LR=0.000050, Time=42.8s\n",
            "  Epoch 13: Train=9.2067%, Val=5.6061%, LR=0.000050, Time=34.4s\n",
            "  Epoch 16: Train=8.0596%, Val=5.1634%, LR=0.000050, Time=34.5s\n",
            "  Epoch 18: Train=7.7078%, Val=4.8621%, LR=0.000050, Time=34.5s\n",
            "  Epoch 19: Train=7.4478%, Val=4.8303%, LR=0.000050, Time=43.0s\n",
            "  Epoch 21: Train=7.2163%, Val=4.5609%, LR=0.000050, Time=34.5s\n",
            "  Epoch 24: Train=6.8721%, Val=4.5066%, LR=0.000050, Time=34.0s\n",
            "  Epoch 25: Train=6.7960%, Val=4.4968%, LR=0.000050, Time=42.9s\n",
            "  Epoch 26: Train=6.7341%, Val=5.0653%, LR=0.000050, Time=42.6s\n",
            "  Epoch 27: Train=6.5670%, Val=4.4382%, LR=0.000050, Time=34.2s\n",
            "  Epoch 29: Train=6.4947%, Val=4.3033%, LR=0.000050, Time=34.2s\n",
            "✅ Fold 3 Seed 456 complete: Best Val MAPE = 4.3033% in 20.1min\n",
            "📈 Fold 3 ensemble results:\n",
            "   Mean MAPE: 4.4274% ± 0.1498%\n",
            "   Best MAPE: 4.3033%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 60.9 minutes\n",
            "\n",
            "🎯 Training Fold 4 with 3 seeds\n",
            "🚀 Training Fold 4 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=136.8803%, Val=38.5134%, LR=0.000050, Time=33.5s\n",
            "  Epoch  2: Train=41.3244%, Val=19.6310%, LR=0.000050, Time=35.9s\n",
            "  Epoch  3: Train=23.8000%, Val=13.5059%, LR=0.000050, Time=43.4s\n",
            "  Epoch  4: Train=18.2185%, Val=10.3529%, LR=0.000050, Time=42.9s\n",
            "  Epoch  5: Train=15.6559%, Val=9.6982%, LR=0.000050, Time=43.2s\n",
            "  Epoch  6: Train=13.5602%, Val=7.7864%, LR=0.000050, Time=42.8s\n",
            "  Epoch  8: Train=11.5158%, Val=7.0144%, LR=0.000050, Time=34.1s\n",
            "  Epoch 10: Train=10.2270%, Val=6.5035%, LR=0.000050, Time=34.4s\n",
            "  Epoch 11: Train=9.6780%, Val=6.0818%, LR=0.000050, Time=42.8s\n",
            "  Epoch 12: Train=9.3705%, Val=5.8428%, LR=0.000050, Time=43.3s\n",
            "  Epoch 13: Train=8.7631%, Val=5.6655%, LR=0.000050, Time=42.4s\n",
            "  Epoch 14: Train=8.5110%, Val=5.4376%, LR=0.000050, Time=42.9s\n",
            "  Epoch 15: Train=8.1230%, Val=5.2073%, LR=0.000050, Time=43.2s\n",
            "  Epoch 16: Train=7.9414%, Val=5.0237%, LR=0.000050, Time=43.3s\n",
            "  Epoch 19: Train=7.3790%, Val=4.8110%, LR=0.000050, Time=34.1s\n",
            "  Epoch 20: Train=7.2179%, Val=4.7046%, LR=0.000050, Time=43.1s\n",
            "  Epoch 21: Train=7.0929%, Val=4.8953%, LR=0.000050, Time=43.3s\n",
            "  Epoch 24: Train=6.7672%, Val=4.4924%, LR=0.000050, Time=34.2s\n",
            "  Epoch 25: Train=6.5771%, Val=4.3985%, LR=0.000050, Time=43.1s\n",
            "  Epoch 26: Train=6.5921%, Val=4.5317%, LR=0.000050, Time=43.2s\n",
            "  Epoch 29: Train=6.2851%, Val=4.1948%, LR=0.000050, Time=34.3s\n",
            "✅ Fold 4 Seed 42 complete: Best Val MAPE = 4.1948% in 20.1min\n",
            "🚀 Training Fold 4 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=94.9601%, Val=34.9315%, LR=0.000050, Time=34.1s\n",
            "  Epoch  2: Train=38.1271%, Val=17.0108%, LR=0.000050, Time=35.4s\n",
            "  Epoch  3: Train=24.1603%, Val=12.8384%, LR=0.000050, Time=42.7s\n",
            "  Epoch  4: Train=18.6667%, Val=11.8353%, LR=0.000050, Time=43.1s\n",
            "  Epoch  5: Train=15.4886%, Val=10.5338%, LR=0.000050, Time=43.4s\n",
            "  Epoch  6: Train=13.8447%, Val=9.2757%, LR=0.000050, Time=43.0s\n",
            "  Epoch  7: Train=12.5911%, Val=8.6864%, LR=0.000050, Time=42.6s\n",
            "  Epoch  8: Train=11.5965%, Val=7.9578%, LR=0.000050, Time=42.8s\n",
            "  Epoch  9: Train=10.9747%, Val=7.4134%, LR=0.000050, Time=43.1s\n",
            "  Epoch 11: Train=9.8117%, Val=6.9189%, LR=0.000050, Time=33.7s\n",
            "  Epoch 12: Train=9.2354%, Val=6.7444%, LR=0.000050, Time=42.7s\n",
            "  Epoch 13: Train=8.8360%, Val=5.9503%, LR=0.000050, Time=42.7s\n",
            "  Epoch 15: Train=8.1993%, Val=5.7781%, LR=0.000050, Time=33.6s\n",
            "  Epoch 16: Train=7.9811%, Val=5.3489%, LR=0.000050, Time=42.4s\n",
            "  Epoch 20: Train=7.2930%, Val=5.2631%, LR=0.000050, Time=34.1s\n",
            "  Epoch 21: Train=7.0987%, Val=5.2635%, LR=0.000050, Time=42.9s\n",
            "  Epoch 22: Train=6.9418%, Val=5.0290%, LR=0.000050, Time=34.0s\n",
            "  Epoch 23: Train=6.9377%, Val=4.7895%, LR=0.000050, Time=42.8s\n",
            "  Epoch 26: Train=6.7062%, Val=4.7998%, LR=0.000050, Time=34.6s\n",
            "  Epoch 28: Train=6.4686%, Val=4.6999%, LR=0.000050, Time=33.6s\n",
            "✅ Fold 4 Seed 123 complete: Best Val MAPE = 4.6999% in 19.9min\n",
            "🚀 Training Fold 4 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=117.2229%, Val=33.5208%, LR=0.000050, Time=33.8s\n",
            "  Epoch  2: Train=42.1642%, Val=19.5232%, LR=0.000050, Time=34.8s\n",
            "  Epoch  3: Train=25.2221%, Val=13.2649%, LR=0.000050, Time=42.5s\n",
            "  Epoch  4: Train=19.3000%, Val=10.8720%, LR=0.000050, Time=42.4s\n",
            "  Epoch  5: Train=16.0769%, Val=9.1960%, LR=0.000050, Time=42.8s\n",
            "  Epoch  6: Train=13.8782%, Val=8.4718%, LR=0.000050, Time=43.1s\n",
            "  Epoch  7: Train=12.8145%, Val=7.4127%, LR=0.000050, Time=42.9s\n",
            "  Epoch  8: Train=11.8271%, Val=7.2071%, LR=0.000050, Time=43.0s\n",
            "  Epoch  9: Train=11.1760%, Val=7.0404%, LR=0.000050, Time=42.5s\n",
            "  Epoch 10: Train=10.5985%, Val=6.2435%, LR=0.000050, Time=43.0s\n",
            "  Epoch 11: Train=10.1502%, Val=6.4039%, LR=0.000050, Time=42.7s\n",
            "  Epoch 14: Train=8.8051%, Val=5.8845%, LR=0.000050, Time=34.3s\n",
            "  Epoch 16: Train=8.1286%, Val=5.0752%, LR=0.000050, Time=33.7s\n",
            "  Epoch 18: Train=7.7288%, Val=4.9190%, LR=0.000050, Time=33.8s\n",
            "  Epoch 21: Train=7.2549%, Val=4.7122%, LR=0.000050, Time=34.3s\n",
            "  Epoch 22: Train=7.1846%, Val=4.6527%, LR=0.000050, Time=42.7s\n",
            "  Epoch 24: Train=6.7908%, Val=4.4313%, LR=0.000050, Time=33.7s\n",
            "  Epoch 26: Train=6.8427%, Val=4.8026%, LR=0.000050, Time=33.8s\n",
            "  Epoch 29: Train=6.3622%, Val=4.3246%, LR=0.000050, Time=33.8s\n",
            "✅ Fold 4 Seed 456 complete: Best Val MAPE = 4.3246% in 19.6min\n",
            "📈 Fold 4 ensemble results:\n",
            "   Mean MAPE: 4.4064% ± 0.2142%\n",
            "   Best MAPE: 4.1948%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 59.7 minutes\n",
            "\n",
            "============================================================\n",
            "📊 COMPREHENSIVE ENSEMBLE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "📋 Individual Fold Analysis:\n",
            "Fold   Best     Mean     Std      Seeds       \n",
            "--------------------------------------------------\n",
            "0      4.2638   4.4438   0.2352   3           \n",
            "1      4.2209   4.3754   0.2127   3           \n",
            "2      4.2856   4.3452   0.0638   3           \n",
            "3      4.3033   4.4274   0.1498   3           \n",
            "4      4.1948   4.4064   0.2142   3           \n",
            "\n",
            "🎯 Overall Performance Summary:\n",
            "   Total models trained: 15\n",
            "   Overall mean MAPE: 4.3996% ± 0.1893%\n",
            "   Best single model: 4.1948%\n",
            "   Expected ensemble MAPE: 4.3996%\n",
            "   Total training time: 6.55 hours\n",
            "   Average time per model: 26.2 minutes\n",
            "\n",
            "📈 Performance Analysis:\n",
            "   Best single fold performance: 4.1948%\n",
            "   Mean of best fold performances: 4.2537%\n",
            "   Stability improvement: 0.1893% standard deviation\n",
            "\n",
            "🏆 Competition Analysis:\n",
            "   Models below 3.0% target: 0/15\n",
            "   Success rate: 0.0%\n",
            "   🎯 Gap to target: 1.1948%\n",
            "💾 Results saved to:\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/ensemble_results.json\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/training_summary.txt\n",
            "\n",
            "✅ Phase 2c Training Complete!\n",
            "Ready for diffusion model development in Phase 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the optimized training for StabilizedUnet and tweaked parameters for performance improvement\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x0K2zUAZLrk",
        "outputId": "f375fb7d-dde2-48d4-a95d-4f348e169245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Phase 2c Optimized Training Pipeline\n",
            "   Target: Sub-3.0% MAPE with stable ensemble\n",
            "   Strategy: 5 folds × 3 seeds = 15 models\n",
            "   Hardware: A100 GPU with batch_size=32\n",
            "🎯 Multi-Seed Trainer initialized\n",
            "   Data: /content/drive/MyDrive/ThinkOnward/Data/Train\n",
            "   Results: /content/drive/MyDrive/ThinkOnward/Result/Phase2c\n",
            "   Seeds: [42, 123, 456]\n",
            "   Total models to train: 15\n",
            "\n",
            "============================================================\n",
            "🚀 STARTING OPTIMIZED MULTI-SEED ENSEMBLE TRAINING\n",
            "============================================================\n",
            "📊 Created 5-fold splits\n",
            "\n",
            "🎯 Training Fold 0 with 3 seeds\n",
            "🚀 Training Fold 0 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b01f5097713b>:19: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  attention = torch.bmm(query, key)\n",
            "<ipython-input-7-b01f5097713b>:22: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  out = torch.bmm(value, attention.permute(0, 2, 1))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  1: Train=81.8903%, Val=26.2064%, LR=0.000010, Time=3347.3s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 9.53e-06\n",
            "  Epoch  2: Train=26.2609%, Val=13.4480%, LR=0.000093, Time=35.1s\n",
            "    📉 Learning rate reduced: 9.53e-06 → 9.29e-05\n",
            "  Epoch  3: Train=19.5851%, Val=10.2425%, LR=0.000100, Time=42.5s\n",
            "    📉 Learning rate reduced: 9.29e-05 → 1.00e-04\n",
            "  Epoch  4: Train=14.3343%, Val=8.7828%, LR=0.000005, Time=42.3s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 4.58e-06\n",
            "  Epoch  5: Train=11.9677%, Val=6.8735%, LR=0.000023, Time=42.2s\n",
            "    📉 Learning rate reduced: 4.58e-06 → 2.30e-05\n",
            "  Epoch  6: Train=11.5396%, Val=6.6761%, LR=0.000026, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.30e-05 → 2.56e-05\n",
            "  Epoch  7: Train=11.1392%, Val=6.5268%, LR=0.000028, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.56e-05 → 2.77e-05\n",
            "  Epoch  8: Train=10.5482%, Val=6.2672%, LR=0.000031, Time=42.4s\n",
            "    📉 Learning rate reduced: 2.77e-05 → 3.13e-05\n",
            "  Epoch  9: Train=10.1990%, Val=6.0243%, LR=0.000035, Time=42.2s\n",
            "    📉 Learning rate reduced: 3.13e-05 → 3.48e-05\n",
            "  Epoch 10: Train=9.5853%, Val=5.8271%, LR=0.000038, Time=41.9s\n",
            "    📉 Learning rate reduced: 3.48e-05 → 3.78e-05\n",
            "  Epoch 11: Train=9.3072%, Val=5.5037%, LR=0.000043, Time=42.5s\n",
            "    📉 Learning rate reduced: 3.78e-05 → 4.27e-05\n",
            "  Epoch 12: Train=8.9120%, Val=5.1176%, LR=0.000049, Time=42.2s\n",
            "    📉 Learning rate reduced: 4.27e-05 → 4.87e-05\n",
            "  Epoch 13: Train=8.3941%, Val=5.0112%, LR=0.000050, Time=42.3s\n",
            "    📉 Learning rate reduced: 4.87e-05 → 5.03e-05\n",
            "    📉 Learning rate reduced: 5.03e-05 → 4.20e-05\n",
            "    📉 Learning rate reduced: 4.20e-05 → 4.98e-05\n",
            "  Epoch 16: Train=7.5200%, Val=4.5542%, LR=0.000057, Time=33.8s\n",
            "    📉 Learning rate reduced: 4.98e-05 → 5.74e-05\n",
            "    📉 Learning rate reduced: 5.74e-05 → 4.78e-05\n",
            "    📉 Learning rate reduced: 4.78e-05 → 5.51e-05\n",
            "  Epoch 19: Train=6.8204%, Val=4.5463%, LR=0.000058, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.51e-05 → 5.75e-05\n",
            "  Epoch 20: Train=6.6043%, Val=4.3575%, LR=0.000060, Time=42.7s\n",
            "    📉 Learning rate reduced: 5.75e-05 → 6.04e-05\n",
            "  Epoch 21: Train=6.6729%, Val=4.3012%, LR=0.000061, Time=42.5s\n",
            "    📉 Learning rate reduced: 6.04e-05 → 6.13e-05\n",
            "    📉 Learning rate reduced: 6.13e-05 → 5.80e-05\n",
            "    📉 Learning rate reduced: 5.80e-05 → 5.57e-05\n",
            "    📉 Learning rate reduced: 5.57e-05 → 6.06e-05\n",
            "  Epoch 25: Train=6.1593%, Val=4.2680%, LR=0.000062, Time=33.7s\n",
            "    📉 Learning rate reduced: 6.06e-05 → 6.18e-05\n",
            "  Epoch 26: Train=5.9345%, Val=4.6584%, LR=0.000056, Time=42.5s\n",
            "    📉 Learning rate reduced: 6.18e-05 → 5.58e-05\n",
            "  Epoch 27: Train=5.8765%, Val=4.1379%, LR=0.000064, Time=34.1s\n",
            "    📉 Learning rate reduced: 5.58e-05 → 6.37e-05\n",
            "    📉 Learning rate reduced: 6.37e-05 → 5.81e-05\n",
            "  Epoch 29: Train=5.6942%, Val=3.8925%, LR=0.000067, Time=34.0s\n",
            "    📉 Learning rate reduced: 5.81e-05 → 6.74e-05\n",
            "    📉 Learning rate reduced: 6.74e-05 → 6.03e-05\n",
            "  Epoch 31: Train=5.5579%, Val=4.3929%, LR=0.000060, Time=33.7s\n",
            "    📉 Learning rate reduced: 6.03e-05 → 5.99e-05\n",
            "    📉 Learning rate reduced: 5.99e-05 → 5.81e-05\n",
            "    📉 Learning rate reduced: 5.81e-05 → 6.72e-05\n",
            "    📉 Learning rate reduced: 6.72e-05 → 6.22e-05\n",
            "    📉 Learning rate reduced: 6.22e-05 → 6.22e-05\n",
            "✅ Fold 0 Seed 42 complete: Best Val MAPE = 3.8925% in 78.1min\n",
            "🚀 Training Fold 0 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=59.9879%, Val=25.1693%, LR=0.000015, Time=34.0s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.46e-05\n",
            "  Epoch  2: Train=24.7879%, Val=13.3640%, LR=0.000093, Time=34.8s\n",
            "    📉 Learning rate reduced: 1.46e-05 → 9.32e-05\n",
            "  Epoch  3: Train=18.8954%, Val=10.1589%, LR=0.000100, Time=42.8s\n",
            "    📉 Learning rate reduced: 9.32e-05 → 1.00e-04\n",
            "  Epoch  4: Train=14.0549%, Val=9.2909%, LR=0.000002, Time=42.6s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 2.22e-06\n",
            "  Epoch  5: Train=12.0648%, Val=7.4735%, LR=0.000016, Time=43.0s\n",
            "    📉 Learning rate reduced: 2.22e-06 → 1.58e-05\n",
            "  Epoch  6: Train=11.6646%, Val=7.4948%, LR=0.000016, Time=42.1s\n",
            "    📉 Learning rate reduced: 1.58e-05 → 1.56e-05\n",
            "  Epoch  7: Train=11.2420%, Val=6.9237%, LR=0.000022, Time=33.8s\n",
            "    📉 Learning rate reduced: 1.56e-05 → 2.24e-05\n",
            "  Epoch  8: Train=10.8373%, Val=6.7975%, LR=0.000024, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.24e-05 → 2.40e-05\n",
            "  Epoch  9: Train=10.5089%, Val=6.7665%, LR=0.000024, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.40e-05 → 2.44e-05\n",
            "  Epoch 10: Train=10.1203%, Val=6.3766%, LR=0.000030, Time=43.0s\n",
            "    📉 Learning rate reduced: 2.44e-05 → 2.98e-05\n",
            "  Epoch 11: Train=9.6548%, Val=6.2014%, LR=0.000032, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.98e-05 → 3.23e-05\n",
            "  Epoch 12: Train=9.0984%, Val=5.8223%, LR=0.000038, Time=43.3s\n",
            "    📉 Learning rate reduced: 3.23e-05 → 3.79e-05\n",
            "  Epoch 13: Train=8.7340%, Val=5.3983%, LR=0.000044, Time=42.3s\n",
            "    📉 Learning rate reduced: 3.79e-05 → 4.43e-05\n",
            "    📉 Learning rate reduced: 4.43e-05 → 4.05e-05\n",
            "    📉 Learning rate reduced: 4.05e-05 → 3.08e-05\n",
            "  Epoch 16: Train=7.9138%, Val=5.2126%, LR=0.000047, Time=34.4s\n",
            "    📉 Learning rate reduced: 3.08e-05 → 4.72e-05\n",
            "    📉 Learning rate reduced: 4.72e-05 → 4.70e-05\n",
            "  Epoch 18: Train=7.4149%, Val=4.9971%, LR=0.000051, Time=34.8s\n",
            "    📉 Learning rate reduced: 4.70e-05 → 5.05e-05\n",
            "    📉 Learning rate reduced: 5.05e-05 → 3.72e-05\n",
            "  Epoch 20: Train=6.9058%, Val=4.7372%, LR=0.000055, Time=33.8s\n",
            "    📉 Learning rate reduced: 3.72e-05 → 5.46e-05\n",
            "  Epoch 21: Train=6.8060%, Val=4.9476%, LR=0.000051, Time=42.8s\n",
            "    📉 Learning rate reduced: 5.46e-05 → 5.13e-05\n",
            "    📉 Learning rate reduced: 5.13e-05 → 5.22e-05\n",
            "    📉 Learning rate reduced: 5.22e-05 → 5.44e-05\n",
            "    📉 Learning rate reduced: 5.44e-05 → 4.02e-05\n",
            "  Epoch 25: Train=6.2798%, Val=4.6152%, LR=0.000056, Time=33.8s\n",
            "    📉 Learning rate reduced: 4.02e-05 → 5.65e-05\n",
            "  Epoch 26: Train=6.3414%, Val=4.4124%, LR=0.000060, Time=42.8s\n",
            "    📉 Learning rate reduced: 5.65e-05 → 5.96e-05\n",
            "    📉 Learning rate reduced: 5.96e-05 → 5.67e-05\n",
            "    📉 Learning rate reduced: 5.67e-05 → 5.11e-05\n",
            "    📉 Learning rate reduced: 5.11e-05 → 5.66e-05\n",
            "  Epoch 30: Train=5.9484%, Val=4.2901%, LR=0.000061, Time=34.0s\n",
            "    📉 Learning rate reduced: 5.66e-05 → 6.14e-05\n",
            "  Epoch 31: Train=5.9398%, Val=4.1057%, LR=0.000064, Time=42.6s\n",
            "    📉 Learning rate reduced: 6.14e-05 → 6.42e-05\n",
            "    📉 Learning rate reduced: 6.42e-05 → 6.35e-05\n",
            "    📉 Learning rate reduced: 6.35e-05 → 5.65e-05\n",
            "    📉 Learning rate reduced: 5.65e-05 → 5.54e-05\n",
            "    📉 Learning rate reduced: 5.54e-05 → 6.42e-05\n",
            "✅ Fold 0 Seed 123 complete: Best Val MAPE = 4.1057% in 22.7min\n",
            "🚀 Training Fold 0 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=72.6345%, Val=26.9079%, LR=0.000007, Time=33.6s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 6.72e-06\n",
            "  Epoch  2: Train=27.2868%, Val=15.4638%, LR=0.000083, Time=34.0s\n",
            "    📉 Learning rate reduced: 6.72e-06 → 8.29e-05\n",
            "  Epoch  3: Train=20.4402%, Val=10.6448%, LR=0.000100, Time=42.7s\n",
            "    📉 Learning rate reduced: 8.29e-05 → 9.97e-05\n",
            "  Epoch  4: Train=14.6999%, Val=9.1385%, LR=0.000003, Time=42.9s\n",
            "    📉 Learning rate reduced: 9.97e-05 → 2.80e-06\n",
            "  Epoch  5: Train=12.5426%, Val=7.1002%, LR=0.000020, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.80e-06 → 2.02e-05\n",
            "  Epoch  6: Train=12.0510%, Val=6.8096%, LR=0.000024, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.02e-05 → 2.39e-05\n",
            "  Epoch  7: Train=11.5335%, Val=6.6646%, LR=0.000026, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.39e-05 → 2.58e-05\n",
            "    📉 Learning rate reduced: 2.58e-05 → 2.36e-05\n",
            "  Epoch  9: Train=10.5548%, Val=6.1592%, LR=0.000033, Time=33.8s\n",
            "    📉 Learning rate reduced: 2.36e-05 → 3.29e-05\n",
            "    📉 Learning rate reduced: 3.29e-05 → 3.20e-05\n",
            "  Epoch 11: Train=9.6339%, Val=6.0307%, LR=0.000035, Time=33.7s\n",
            "    📉 Learning rate reduced: 3.20e-05 → 3.47e-05\n",
            "  Epoch 12: Train=9.2406%, Val=5.6034%, LR=0.000041, Time=42.8s\n",
            "    📉 Learning rate reduced: 3.47e-05 → 4.12e-05\n",
            "    📉 Learning rate reduced: 4.12e-05 → 3.72e-05\n",
            "  Epoch 14: Train=8.4515%, Val=5.2632%, LR=0.000046, Time=34.5s\n",
            "    📉 Learning rate reduced: 3.72e-05 → 4.64e-05\n",
            "  Epoch 15: Train=8.1812%, Val=5.1137%, LR=0.000049, Time=42.8s\n",
            "    📉 Learning rate reduced: 4.64e-05 → 4.87e-05\n",
            "  Epoch 16: Train=7.9153%, Val=5.0242%, LR=0.000050, Time=43.4s\n",
            "    📉 Learning rate reduced: 4.87e-05 → 5.01e-05\n",
            "  Epoch 17: Train=7.5572%, Val=4.9462%, LR=0.000051, Time=43.0s\n",
            "    📉 Learning rate reduced: 5.01e-05 → 5.13e-05\n",
            "  Epoch 18: Train=7.3102%, Val=4.6869%, LR=0.000055, Time=42.6s\n",
            "    📉 Learning rate reduced: 5.13e-05 → 5.54e-05\n",
            "    📉 Learning rate reduced: 5.54e-05 → 5.17e-05\n",
            "    📉 Learning rate reduced: 5.17e-05 → 4.94e-05\n",
            "  Epoch 21: Train=6.6907%, Val=4.4315%, LR=0.000059, Time=33.9s\n",
            "    📉 Learning rate reduced: 4.94e-05 → 5.93e-05\n",
            "    📉 Learning rate reduced: 5.93e-05 → 5.83e-05\n",
            "    📉 Learning rate reduced: 5.83e-05 → 5.80e-05\n",
            "  Epoch 24: Train=6.3616%, Val=4.1186%, LR=0.000064, Time=33.9s\n",
            "    📉 Learning rate reduced: 5.80e-05 → 6.40e-05\n",
            "    📉 Learning rate reduced: 6.40e-05 → 5.77e-05\n",
            "  Epoch 26: Train=6.1080%, Val=4.3225%, LR=0.000061, Time=34.2s\n",
            "    📉 Learning rate reduced: 5.77e-05 → 6.10e-05\n",
            "    📉 Learning rate reduced: 6.10e-05 → 6.34e-05\n",
            "    📉 Learning rate reduced: 6.34e-05 → 6.15e-05\n",
            "    📉 Learning rate reduced: 6.15e-05 → 5.86e-05\n",
            "    📉 Learning rate reduced: 5.86e-05 → 6.07e-05\n",
            "  Epoch 31: Train=5.8074%, Val=3.9362%, LR=0.000067, Time=33.9s\n",
            "    📉 Learning rate reduced: 6.07e-05 → 6.67e-05\n",
            "    📉 Learning rate reduced: 6.67e-05 → 6.23e-05\n",
            "    📉 Learning rate reduced: 6.23e-05 → 5.80e-05\n",
            "  Epoch 34: Train=5.4777%, Val=3.9147%, LR=0.000067, Time=33.9s\n",
            "    📉 Learning rate reduced: 5.80e-05 → 6.71e-05\n",
            "    📉 Learning rate reduced: 6.71e-05 → 6.64e-05\n",
            "✅ Fold 0 Seed 456 complete: Best Val MAPE = 3.9147% in 22.8min\n",
            "📈 Fold 0 ensemble results:\n",
            "   Mean MAPE: 3.9710% ± 0.0957%\n",
            "   Best MAPE: 3.8925%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 123.7 minutes\n",
            "\n",
            "🎯 Training Fold 1 with 3 seeds\n",
            "🚀 Training Fold 1 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=81.4729%, Val=27.9409%, LR=0.000004, Time=34.1s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 3.57e-06\n",
            "  Epoch  2: Train=26.3048%, Val=14.2203%, LR=0.000090, Time=35.3s\n",
            "    📉 Learning rate reduced: 3.57e-06 → 8.95e-05\n",
            "  Epoch  3: Train=19.5845%, Val=9.9839%, LR=0.000001, Time=42.4s\n",
            "    📉 Learning rate reduced: 8.95e-05 → 1.00e-06\n",
            "  Epoch  4: Train=15.7497%, Val=8.3223%, LR=0.000008, Time=43.1s\n",
            "    📉 Learning rate reduced: 1.00e-06 → 7.72e-06\n",
            "  Epoch  5: Train=15.1124%, Val=8.1891%, LR=0.000009, Time=42.8s\n",
            "    📉 Learning rate reduced: 7.72e-06 → 8.80e-06\n",
            "  Epoch  6: Train=14.3870%, Val=7.5189%, LR=0.000015, Time=42.7s\n",
            "    📉 Learning rate reduced: 8.80e-06 → 1.53e-05\n",
            "  Epoch  7: Train=13.7675%, Val=7.1516%, LR=0.000020, Time=42.5s\n",
            "    📉 Learning rate reduced: 1.53e-05 → 1.95e-05\n",
            "  Epoch  8: Train=12.9437%, Val=6.8100%, LR=0.000024, Time=42.3s\n",
            "    📉 Learning rate reduced: 1.95e-05 → 2.38e-05\n",
            "  Epoch  9: Train=12.1447%, Val=6.7671%, LR=0.000024, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.38e-05 → 2.44e-05\n",
            "  Epoch 10: Train=11.4398%, Val=6.3843%, LR=0.000030, Time=42.2s\n",
            "    📉 Learning rate reduced: 2.44e-05 → 2.96e-05\n",
            "  Epoch 11: Train=10.9000%, Val=6.0597%, LR=0.000034, Time=42.6s\n",
            "    📉 Learning rate reduced: 2.96e-05 → 3.43e-05\n",
            "  Epoch 12: Train=10.3892%, Val=5.9029%, LR=0.000037, Time=42.0s\n",
            "    📉 Learning rate reduced: 3.43e-05 → 3.66e-05\n",
            "  Epoch 13: Train=9.7829%, Val=5.7232%, LR=0.000039, Time=42.4s\n",
            "    📉 Learning rate reduced: 3.66e-05 → 3.94e-05\n",
            "    📉 Learning rate reduced: 3.94e-05 → 3.77e-05\n",
            "  Epoch 15: Train=8.9638%, Val=5.1717%, LR=0.000048, Time=34.1s\n",
            "    📉 Learning rate reduced: 3.77e-05 → 4.78e-05\n",
            "  Epoch 16: Train=8.6893%, Val=4.9765%, LR=0.000051, Time=42.7s\n",
            "    📉 Learning rate reduced: 4.78e-05 → 5.09e-05\n",
            "  Epoch 17: Train=8.3268%, Val=4.7909%, LR=0.000054, Time=42.7s\n",
            "    📉 Learning rate reduced: 5.09e-05 → 5.37e-05\n",
            "    📉 Learning rate reduced: 5.37e-05 → 5.25e-05\n",
            "  Epoch 19: Train=7.4738%, Val=4.7220%, LR=0.000055, Time=34.6s\n",
            "    📉 Learning rate reduced: 5.25e-05 → 5.48e-05\n",
            "  Epoch 20: Train=7.2886%, Val=4.5272%, LR=0.000058, Time=42.5s\n",
            "    📉 Learning rate reduced: 5.48e-05 → 5.78e-05\n",
            "  Epoch 21: Train=7.0983%, Val=4.5399%, LR=0.000058, Time=42.7s\n",
            "    📉 Learning rate reduced: 5.78e-05 → 5.76e-05\n",
            "  Epoch 22: Train=6.9936%, Val=4.2688%, LR=0.000062, Time=34.3s\n",
            "    📉 Learning rate reduced: 5.76e-05 → 6.18e-05\n",
            "    📉 Learning rate reduced: 6.18e-05 → 6.03e-05\n",
            "    📉 Learning rate reduced: 6.03e-05 → 4.97e-05\n",
            "    📉 Learning rate reduced: 4.97e-05 → 6.04e-05\n",
            "  Epoch 26: Train=6.3623%, Val=4.1178%, LR=0.000064, Time=34.2s\n",
            "    📉 Learning rate reduced: 6.04e-05 → 6.40e-05\n",
            "    📉 Learning rate reduced: 6.40e-05 → 5.87e-05\n",
            "    📉 Learning rate reduced: 5.87e-05 → 5.96e-05\n",
            "  Epoch 29: Train=6.0087%, Val=3.9927%, LR=0.000066, Time=34.9s\n",
            "    📉 Learning rate reduced: 5.96e-05 → 6.59e-05\n",
            "    📉 Learning rate reduced: 6.59e-05 → 6.08e-05\n",
            "  Epoch 31: Train=5.8393%, Val=4.1117%, LR=0.000064, Time=34.1s\n",
            "    📉 Learning rate reduced: 6.08e-05 → 6.41e-05\n",
            "    📉 Learning rate reduced: 6.41e-05 → 6.19e-05\n",
            "    📉 Learning rate reduced: 6.19e-05 → 6.43e-05\n",
            "    📉 Learning rate reduced: 6.43e-05 → 5.52e-05\n",
            "    📉 Learning rate reduced: 5.52e-05 → 6.52e-05\n",
            "✅ Fold 1 Seed 42 complete: Best Val MAPE = 3.9927% in 23.1min\n",
            "🚀 Training Fold 1 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=60.2761%, Val=25.5191%, LR=0.000013, Time=34.5s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.28e-05\n",
            "  Epoch  2: Train=24.9779%, Val=13.4441%, LR=0.000093, Time=35.2s\n",
            "    📉 Learning rate reduced: 1.28e-05 → 9.29e-05\n",
            "  Epoch  3: Train=19.1517%, Val=10.6558%, LR=0.000100, Time=42.3s\n",
            "    📉 Learning rate reduced: 9.29e-05 → 9.97e-05\n",
            "  Epoch  4: Train=14.2626%, Val=8.6843%, LR=0.000005, Time=42.2s\n",
            "    📉 Learning rate reduced: 9.97e-05 → 5.17e-06\n",
            "  Epoch  5: Train=12.2826%, Val=7.0183%, LR=0.000021, Time=42.9s\n",
            "    📉 Learning rate reduced: 5.17e-06 → 2.12e-05\n",
            "  Epoch  6: Train=11.8609%, Val=7.1054%, LR=0.000020, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.12e-05 → 2.01e-05\n",
            "  Epoch  7: Train=11.2645%, Val=6.6450%, LR=0.000026, Time=33.9s\n",
            "    📉 Learning rate reduced: 2.01e-05 → 2.60e-05\n",
            "  Epoch  8: Train=10.7294%, Val=6.6132%, LR=0.000026, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.60e-05 → 2.65e-05\n",
            "  Epoch  9: Train=10.4232%, Val=6.4616%, LR=0.000029, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.65e-05 → 2.86e-05\n",
            "  Epoch 10: Train=9.9865%, Val=5.9804%, LR=0.000035, Time=42.4s\n",
            "    📉 Learning rate reduced: 2.86e-05 → 3.55e-05\n",
            "  Epoch 11: Train=9.3749%, Val=5.6959%, LR=0.000040, Time=43.0s\n",
            "    📉 Learning rate reduced: 3.55e-05 → 3.98e-05\n",
            "    📉 Learning rate reduced: 3.98e-05 → 3.93e-05\n",
            "  Epoch 13: Train=8.6000%, Val=5.2880%, LR=0.000046, Time=33.8s\n",
            "    📉 Learning rate reduced: 3.93e-05 → 4.60e-05\n",
            "  Epoch 14: Train=8.3053%, Val=5.2430%, LR=0.000047, Time=42.4s\n",
            "    📉 Learning rate reduced: 4.60e-05 → 4.67e-05\n",
            "  Epoch 15: Train=7.8689%, Val=5.0343%, LR=0.000050, Time=42.3s\n",
            "    📉 Learning rate reduced: 4.67e-05 → 5.00e-05\n",
            "  Epoch 16: Train=7.8753%, Val=5.3315%, LR=0.000045, Time=42.8s\n",
            "    📉 Learning rate reduced: 5.00e-05 → 4.54e-05\n",
            "  Epoch 17: Train=7.3877%, Val=4.8552%, LR=0.000053, Time=33.6s\n",
            "    📉 Learning rate reduced: 4.54e-05 → 5.28e-05\n",
            "    📉 Learning rate reduced: 5.28e-05 → 4.67e-05\n",
            "    📉 Learning rate reduced: 4.67e-05 → 4.03e-05\n",
            "  Epoch 20: Train=6.8232%, Val=4.4106%, LR=0.000060, Time=33.8s\n",
            "    📉 Learning rate reduced: 4.03e-05 → 5.96e-05\n",
            "  Epoch 21: Train=6.7748%, Val=4.7215%, LR=0.000055, Time=42.5s\n",
            "    📉 Learning rate reduced: 5.96e-05 → 5.48e-05\n",
            "    📉 Learning rate reduced: 5.48e-05 → 5.38e-05\n",
            "    📉 Learning rate reduced: 5.38e-05 → 5.48e-05\n",
            "    📉 Learning rate reduced: 5.48e-05 → 4.92e-05\n",
            "    📉 Learning rate reduced: 4.92e-05 → 5.54e-05\n",
            "  Epoch 26: Train=6.2082%, Val=4.0789%, LR=0.000065, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.54e-05 → 6.46e-05\n",
            "    📉 Learning rate reduced: 6.46e-05 → 5.52e-05\n",
            "    📉 Learning rate reduced: 5.52e-05 → 6.07e-05\n",
            "    📉 Learning rate reduced: 6.07e-05 → 6.17e-05\n",
            "    📉 Learning rate reduced: 6.17e-05 → 5.98e-05\n",
            "  Epoch 31: Train=5.9838%, Val=4.7111%, LR=0.000055, Time=33.5s\n",
            "    📉 Learning rate reduced: 5.98e-05 → 5.50e-05\n",
            "  Epoch 32: Train=5.7768%, Val=3.9801%, LR=0.000066, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.50e-05 → 6.61e-05\n",
            "    📉 Learning rate reduced: 6.61e-05 → 5.72e-05\n",
            "    📉 Learning rate reduced: 5.72e-05 → 4.69e-05\n",
            "    📉 Learning rate reduced: 4.69e-05 → 6.53e-05\n",
            "✅ Fold 1 Seed 123 complete: Best Val MAPE = 3.9801% in 22.4min\n",
            "🚀 Training Fold 1 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=72.8387%, Val=26.2399%, LR=0.000009, Time=33.7s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 9.39e-06\n",
            "  Epoch  2: Train=26.8877%, Val=14.7616%, LR=0.000087, Time=35.0s\n",
            "    📉 Learning rate reduced: 9.39e-06 → 8.68e-05\n",
            "  Epoch  3: Train=20.0188%, Val=9.8817%, LR=0.000001, Time=42.7s\n",
            "    📉 Learning rate reduced: 8.68e-05 → 1.03e-06\n",
            "  Epoch  4: Train=15.9932%, Val=8.5573%, LR=0.000006, Time=42.1s\n",
            "    📉 Learning rate reduced: 1.03e-06 → 6.00e-06\n",
            "  Epoch  5: Train=15.5212%, Val=8.1275%, LR=0.000009, Time=42.6s\n",
            "    📉 Learning rate reduced: 6.00e-06 → 9.32e-06\n",
            "  Epoch  6: Train=14.9354%, Val=7.8149%, LR=0.000012, Time=42.4s\n",
            "    📉 Learning rate reduced: 9.32e-06 → 1.22e-05\n",
            "  Epoch  7: Train=14.1622%, Val=7.5149%, LR=0.000015, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.22e-05 → 1.53e-05\n",
            "  Epoch  8: Train=13.5215%, Val=7.1885%, LR=0.000019, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.53e-05 → 1.91e-05\n",
            "  Epoch  9: Train=12.9145%, Val=6.8407%, LR=0.000023, Time=42.9s\n",
            "    📉 Learning rate reduced: 1.91e-05 → 2.34e-05\n",
            "  Epoch 10: Train=12.2336%, Val=6.5020%, LR=0.000028, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.34e-05 → 2.80e-05\n",
            "  Epoch 11: Train=11.6995%, Val=6.2489%, LR=0.000032, Time=42.6s\n",
            "    📉 Learning rate reduced: 2.80e-05 → 3.16e-05\n",
            "    📉 Learning rate reduced: 3.16e-05 → 2.96e-05\n",
            "  Epoch 13: Train=10.5355%, Val=6.2467%, LR=0.000032, Time=34.4s\n",
            "    📉 Learning rate reduced: 2.96e-05 → 3.16e-05\n",
            "  Epoch 14: Train=9.9696%, Val=5.6657%, LR=0.000040, Time=42.6s\n",
            "    📉 Learning rate reduced: 3.16e-05 → 4.02e-05\n",
            "  Epoch 15: Train=9.6336%, Val=5.5927%, LR=0.000041, Time=42.8s\n",
            "    📉 Learning rate reduced: 4.02e-05 → 4.13e-05\n",
            "  Epoch 16: Train=9.1053%, Val=5.8713%, LR=0.000037, Time=42.8s\n",
            "    📉 Learning rate reduced: 4.13e-05 → 3.71e-05\n",
            "  Epoch 17: Train=8.7062%, Val=5.4372%, LR=0.000044, Time=34.1s\n",
            "    📉 Learning rate reduced: 3.71e-05 → 4.37e-05\n",
            "  Epoch 18: Train=8.4299%, Val=5.2617%, LR=0.000046, Time=42.9s\n",
            "    📉 Learning rate reduced: 4.37e-05 → 4.64e-05\n",
            "  Epoch 19: Train=8.0739%, Val=4.8468%, LR=0.000053, Time=42.5s\n",
            "    📉 Learning rate reduced: 4.64e-05 → 5.29e-05\n",
            "    📉 Learning rate reduced: 5.29e-05 → 4.78e-05\n",
            "  Epoch 21: Train=7.5129%, Val=4.9692%, LR=0.000051, Time=34.1s\n",
            "    📉 Learning rate reduced: 4.78e-05 → 5.10e-05\n",
            "  Epoch 22: Train=7.3027%, Val=4.6446%, LR=0.000056, Time=33.3s\n",
            "    📉 Learning rate reduced: 5.10e-05 → 5.60e-05\n",
            "    📉 Learning rate reduced: 5.60e-05 → 5.54e-05\n",
            "  Epoch 24: Train=6.9034%, Val=4.3835%, LR=0.000060, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.54e-05 → 6.00e-05\n",
            "    📉 Learning rate reduced: 6.00e-05 → 4.96e-05\n",
            "  Epoch 26: Train=6.6505%, Val=4.3554%, LR=0.000060, Time=33.8s\n",
            "    📉 Learning rate reduced: 4.96e-05 → 6.05e-05\n",
            "  Epoch 27: Train=6.5018%, Val=4.1596%, LR=0.000063, Time=42.7s\n",
            "    📉 Learning rate reduced: 6.05e-05 → 6.34e-05\n",
            "    📉 Learning rate reduced: 6.34e-05 → 5.87e-05\n",
            "    📉 Learning rate reduced: 5.87e-05 → 6.19e-05\n",
            "  Epoch 30: Train=6.0373%, Val=4.1168%, LR=0.000064, Time=33.9s\n",
            "    📉 Learning rate reduced: 6.19e-05 → 6.41e-05\n",
            "  Epoch 31: Train=6.0640%, Val=3.9774%, LR=0.000066, Time=42.5s\n",
            "    📉 Learning rate reduced: 6.41e-05 → 6.61e-05\n",
            "    📉 Learning rate reduced: 6.61e-05 → 5.80e-05\n",
            "    📉 Learning rate reduced: 5.80e-05 → 6.11e-05\n",
            "    📉 Learning rate reduced: 6.11e-05 → 6.40e-05\n",
            "  Epoch 35: Train=5.6866%, Val=3.8839%, LR=0.000068, Time=33.5s\n",
            "    📉 Learning rate reduced: 6.40e-05 → 6.75e-05\n",
            "✅ Fold 1 Seed 456 complete: Best Val MAPE = 3.8839% in 23.4min\n",
            "📈 Fold 1 ensemble results:\n",
            "   Mean MAPE: 3.9522% ± 0.0486%\n",
            "   Best MAPE: 3.8839%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 68.9 minutes\n",
            "\n",
            "🎯 Training Fold 2 with 3 seeds\n",
            "🚀 Training Fold 2 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=81.0541%, Val=27.9986%, LR=0.000003, Time=41.0s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 3.43e-06\n",
            "  Epoch  2: Train=25.9912%, Val=14.7964%, LR=0.000087, Time=35.1s\n",
            "    📉 Learning rate reduced: 3.43e-06 → 8.66e-05\n",
            "  Epoch  3: Train=19.3368%, Val=12.2691%, LR=0.000097, Time=42.0s\n",
            "    📉 Learning rate reduced: 8.66e-05 → 9.69e-05\n",
            "  Epoch  4: Train=14.1256%, Val=9.4359%, LR=0.000002, Time=42.2s\n",
            "    📉 Learning rate reduced: 9.69e-05 → 1.78e-06\n",
            "  Epoch  5: Train=12.0177%, Val=7.0645%, LR=0.000021, Time=42.3s\n",
            "    📉 Learning rate reduced: 1.78e-06 → 2.06e-05\n",
            "  Epoch  6: Train=11.4661%, Val=6.7299%, LR=0.000025, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.06e-05 → 2.49e-05\n",
            "  Epoch  7: Train=11.1156%, Val=6.7048%, LR=0.000025, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.49e-05 → 2.52e-05\n",
            "  Epoch  8: Train=10.4906%, Val=6.3379%, LR=0.000030, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.52e-05 → 3.03e-05\n",
            "  Epoch  9: Train=10.1312%, Val=6.2443%, LR=0.000032, Time=42.6s\n",
            "    📉 Learning rate reduced: 3.03e-05 → 3.16e-05\n",
            "  Epoch 10: Train=9.6861%, Val=6.1227%, LR=0.000033, Time=42.2s\n",
            "    📉 Learning rate reduced: 3.16e-05 → 3.34e-05\n",
            "  Epoch 11: Train=9.4163%, Val=5.8247%, LR=0.000038, Time=41.8s\n",
            "    📉 Learning rate reduced: 3.34e-05 → 3.78e-05\n",
            "  Epoch 12: Train=8.9967%, Val=5.5477%, LR=0.000042, Time=42.2s\n",
            "    📉 Learning rate reduced: 3.78e-05 → 4.20e-05\n",
            "  Epoch 13: Train=8.5600%, Val=5.1893%, LR=0.000048, Time=42.3s\n",
            "    📉 Learning rate reduced: 4.20e-05 → 4.76e-05\n",
            "    📉 Learning rate reduced: 4.76e-05 → 4.71e-05\n",
            "  Epoch 15: Train=7.8909%, Val=4.8569%, LR=0.000053, Time=33.3s\n",
            "    📉 Learning rate reduced: 4.71e-05 → 5.27e-05\n",
            "  Epoch 16: Train=7.5815%, Val=4.7379%, LR=0.000055, Time=42.7s\n",
            "    📉 Learning rate reduced: 5.27e-05 → 5.46e-05\n",
            "    📉 Learning rate reduced: 5.46e-05 → 5.32e-05\n",
            "    📉 Learning rate reduced: 5.32e-05 → 4.79e-05\n",
            "  Epoch 19: Train=6.8495%, Val=4.6250%, LR=0.000056, Time=33.4s\n",
            "    📉 Learning rate reduced: 4.79e-05 → 5.63e-05\n",
            "  Epoch 20: Train=6.7436%, Val=4.3963%, LR=0.000060, Time=42.4s\n",
            "    📉 Learning rate reduced: 5.63e-05 → 5.98e-05\n",
            "  Epoch 21: Train=6.6388%, Val=4.7146%, LR=0.000055, Time=42.8s\n",
            "    📉 Learning rate reduced: 5.98e-05 → 5.49e-05\n",
            "  Epoch 22: Train=6.3612%, Val=4.3538%, LR=0.000060, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.49e-05 → 6.05e-05\n",
            "    📉 Learning rate reduced: 6.05e-05 → 5.75e-05\n",
            "    📉 Learning rate reduced: 5.75e-05 → 5.92e-05\n",
            "  Epoch 25: Train=6.1302%, Val=4.2404%, LR=0.000062, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.92e-05 → 6.22e-05\n",
            "  Epoch 26: Train=5.9765%, Val=4.0487%, LR=0.000065, Time=42.6s\n",
            "    📉 Learning rate reduced: 6.22e-05 → 6.51e-05\n",
            "    📉 Learning rate reduced: 6.51e-05 → 6.07e-05\n",
            "    📉 Learning rate reduced: 6.07e-05 → 6.19e-05\n",
            "    📉 Learning rate reduced: 6.19e-05 → 6.29e-05\n",
            "    📉 Learning rate reduced: 6.29e-05 → 5.21e-05\n",
            "  Epoch 31: Train=5.6325%, Val=4.2251%, LR=0.000062, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.21e-05 → 6.24e-05\n",
            "    📉 Learning rate reduced: 6.24e-05 → 6.15e-05\n",
            "    📉 Learning rate reduced: 6.15e-05 → 6.47e-05\n",
            "    📉 Learning rate reduced: 6.47e-05 → 5.10e-05\n",
            "    ⏹️  Early stopping at epoch 34\n",
            "✅ Fold 2 Seed 42 complete: Best Val MAPE = 4.0487% in 22.3min\n",
            "🚀 Training Fold 2 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=60.2857%, Val=25.8099%, LR=0.000011, Time=34.0s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.13e-05\n",
            "  Epoch  2: Train=25.4712%, Val=13.8527%, LR=0.000091, Time=35.3s\n",
            "    📉 Learning rate reduced: 1.13e-05 → 9.12e-05\n",
            "  Epoch  3: Train=19.3518%, Val=10.9938%, LR=0.000099, Time=42.9s\n",
            "    📉 Learning rate reduced: 9.12e-05 → 9.94e-05\n",
            "  Epoch  4: Train=14.7884%, Val=9.1261%, LR=0.000003, Time=42.8s\n",
            "    📉 Learning rate reduced: 9.94e-05 → 2.85e-06\n",
            "  Epoch  5: Train=12.6565%, Val=7.5679%, LR=0.000015, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.85e-06 → 1.48e-05\n",
            "  Epoch  6: Train=12.1248%, Val=7.2004%, LR=0.000019, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.48e-05 → 1.89e-05\n",
            "  Epoch  7: Train=11.7149%, Val=7.1455%, LR=0.000020, Time=43.4s\n",
            "    📉 Learning rate reduced: 1.89e-05 → 1.96e-05\n",
            "  Epoch  8: Train=11.0925%, Val=6.8341%, LR=0.000024, Time=42.4s\n",
            "    📉 Learning rate reduced: 1.96e-05 → 2.35e-05\n",
            "  Epoch  9: Train=10.8052%, Val=6.7460%, LR=0.000025, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.35e-05 → 2.47e-05\n",
            "  Epoch 10: Train=10.4294%, Val=6.7122%, LR=0.000025, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.47e-05 → 2.51e-05\n",
            "  Epoch 11: Train=9.8430%, Val=6.5472%, LR=0.000027, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.51e-05 → 2.74e-05\n",
            "  Epoch 12: Train=9.5568%, Val=6.1295%, LR=0.000033, Time=42.9s\n",
            "    📉 Learning rate reduced: 2.74e-05 → 3.33e-05\n",
            "    📉 Learning rate reduced: 3.33e-05 → 3.01e-05\n",
            "  Epoch 14: Train=8.6877%, Val=5.9163%, LR=0.000036, Time=33.8s\n",
            "    📉 Learning rate reduced: 3.01e-05 → 3.64e-05\n",
            "  Epoch 15: Train=8.4472%, Val=5.8343%, LR=0.000038, Time=42.8s\n",
            "    📉 Learning rate reduced: 3.64e-05 → 3.77e-05\n",
            "  Epoch 16: Train=8.1472%, Val=5.3673%, LR=0.000045, Time=42.3s\n",
            "    📉 Learning rate reduced: 3.77e-05 → 4.48e-05\n",
            "  Epoch 17: Train=7.7627%, Val=5.2097%, LR=0.000047, Time=42.4s\n",
            "    📉 Learning rate reduced: 4.48e-05 → 4.72e-05\n",
            "    📉 Learning rate reduced: 4.72e-05 → 4.47e-05\n",
            "    📉 Learning rate reduced: 4.47e-05 → 3.81e-05\n",
            "  Epoch 20: Train=7.0918%, Val=4.8600%, LR=0.000053, Time=34.2s\n",
            "    📉 Learning rate reduced: 3.81e-05 → 5.27e-05\n",
            "  Epoch 21: Train=7.0370%, Val=4.7462%, LR=0.000054, Time=42.9s\n",
            "    📉 Learning rate reduced: 5.27e-05 → 5.44e-05\n",
            "    📉 Learning rate reduced: 5.44e-05 → 5.29e-05\n",
            "  Epoch 23: Train=6.5637%, Val=4.6403%, LR=0.000056, Time=34.0s\n",
            "    📉 Learning rate reduced: 5.29e-05 → 5.61e-05\n",
            "    📉 Learning rate reduced: 5.61e-05 → 4.78e-05\n",
            "    📉 Learning rate reduced: 4.78e-05 → 5.36e-05\n",
            "  Epoch 26: Train=6.3824%, Val=4.7069%, LR=0.000055, Time=33.5s\n",
            "    📉 Learning rate reduced: 5.36e-05 → 5.51e-05\n",
            "  Epoch 27: Train=6.2211%, Val=4.5474%, LR=0.000058, Time=34.0s\n",
            "    📉 Learning rate reduced: 5.51e-05 → 5.75e-05\n",
            "  Epoch 28: Train=6.4280%, Val=4.2985%, LR=0.000061, Time=43.1s\n",
            "    📉 Learning rate reduced: 5.75e-05 → 6.13e-05\n",
            "    📉 Learning rate reduced: 6.13e-05 → 5.67e-05\n",
            "  Epoch 30: Train=6.1526%, Val=4.2263%, LR=0.000062, Time=34.3s\n",
            "    📉 Learning rate reduced: 5.67e-05 → 6.24e-05\n",
            "  Epoch 31: Train=6.0272%, Val=4.4528%, LR=0.000059, Time=42.9s\n",
            "    📉 Learning rate reduced: 6.24e-05 → 5.90e-05\n",
            "    📉 Learning rate reduced: 5.90e-05 → 6.12e-05\n",
            "    📉 Learning rate reduced: 6.12e-05 → 6.10e-05\n",
            "    📉 Learning rate reduced: 6.10e-05 → 5.54e-05\n",
            "  Epoch 35: Train=5.6584%, Val=4.0185%, LR=0.000066, Time=34.0s\n",
            "    📉 Learning rate reduced: 5.54e-05 → 6.55e-05\n",
            "✅ Fold 2 Seed 123 complete: Best Val MAPE = 4.0185% in 23.3min\n",
            "🚀 Training Fold 2 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=72.6726%, Val=25.8382%, LR=0.000011, Time=42.4s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.12e-05\n",
            "  Epoch  2: Train=26.7048%, Val=14.7863%, LR=0.000087, Time=34.4s\n",
            "    📉 Learning rate reduced: 1.12e-05 → 8.67e-05\n",
            "  Epoch  3: Train=19.9502%, Val=11.5745%, LR=0.000098, Time=42.0s\n",
            "    📉 Learning rate reduced: 8.67e-05 → 9.85e-05\n",
            "  Epoch  4: Train=14.2158%, Val=8.0444%, LR=0.000010, Time=42.7s\n",
            "    📉 Learning rate reduced: 9.85e-05 → 1.01e-05\n",
            "  Epoch  5: Train=11.9822%, Val=6.8514%, LR=0.000023, Time=42.2s\n",
            "    📉 Learning rate reduced: 1.01e-05 → 2.33e-05\n",
            "  Epoch  6: Train=11.4895%, Val=6.7788%, LR=0.000024, Time=43.0s\n",
            "    📉 Learning rate reduced: 2.33e-05 → 2.43e-05\n",
            "  Epoch  7: Train=10.9277%, Val=6.4528%, LR=0.000029, Time=42.0s\n",
            "    📉 Learning rate reduced: 2.43e-05 → 2.87e-05\n",
            "    📉 Learning rate reduced: 2.87e-05 → 2.71e-05\n",
            "  Epoch  9: Train=9.9883%, Val=6.1460%, LR=0.000033, Time=33.4s\n",
            "    📉 Learning rate reduced: 2.71e-05 → 3.31e-05\n",
            "  Epoch 10: Train=9.6537%, Val=5.8150%, LR=0.000038, Time=42.1s\n",
            "    📉 Learning rate reduced: 3.31e-05 → 3.80e-05\n",
            "  Epoch 11: Train=9.0748%, Val=5.6065%, LR=0.000041, Time=42.1s\n",
            "    📉 Learning rate reduced: 3.80e-05 → 4.11e-05\n",
            "    📉 Learning rate reduced: 4.11e-05 → 3.74e-05\n",
            "  Epoch 13: Train=8.4456%, Val=5.3305%, LR=0.000045, Time=33.2s\n",
            "    📉 Learning rate reduced: 3.74e-05 → 4.54e-05\n",
            "    📉 Learning rate reduced: 4.54e-05 → 4.39e-05\n",
            "  Epoch 15: Train=7.8454%, Val=5.2154%, LR=0.000047, Time=33.5s\n",
            "    📉 Learning rate reduced: 4.39e-05 → 4.72e-05\n",
            "  Epoch 16: Train=7.6112%, Val=4.8602%, LR=0.000053, Time=42.7s\n",
            "    📉 Learning rate reduced: 4.72e-05 → 5.27e-05\n",
            "  Epoch 17: Train=7.3407%, Val=4.6690%, LR=0.000056, Time=42.2s\n",
            "    📉 Learning rate reduced: 5.27e-05 → 5.56e-05\n",
            "    📉 Learning rate reduced: 5.56e-05 → 5.00e-05\n",
            "    📉 Learning rate reduced: 5.00e-05 → 5.33e-05\n",
            "  Epoch 20: Train=6.8883%, Val=4.4745%, LR=0.000059, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.33e-05 → 5.86e-05\n",
            "  Epoch 21: Train=6.7132%, Val=4.5317%, LR=0.000058, Time=42.6s\n",
            "    📉 Learning rate reduced: 5.86e-05 → 5.78e-05\n",
            "  Epoch 22: Train=6.5263%, Val=4.4050%, LR=0.000060, Time=33.8s\n",
            "    📉 Learning rate reduced: 5.78e-05 → 5.97e-05\n",
            "  Epoch 23: Train=6.3062%, Val=4.3630%, LR=0.000060, Time=42.2s\n",
            "    📉 Learning rate reduced: 5.97e-05 → 6.03e-05\n",
            "  Epoch 24: Train=6.4335%, Val=4.2179%, LR=0.000063, Time=42.5s\n",
            "    📉 Learning rate reduced: 6.03e-05 → 6.25e-05\n",
            "    📉 Learning rate reduced: 6.25e-05 → 5.90e-05\n",
            "  Epoch 26: Train=6.0477%, Val=4.2002%, LR=0.000063, Time=33.8s\n",
            "    📉 Learning rate reduced: 5.90e-05 → 6.28e-05\n",
            "  Epoch 27: Train=6.1145%, Val=3.9780%, LR=0.000066, Time=42.4s\n",
            "    📉 Learning rate reduced: 6.28e-05 → 6.61e-05\n",
            "    📉 Learning rate reduced: 6.61e-05 → 5.62e-05\n",
            "    📉 Learning rate reduced: 5.62e-05 → 6.11e-05\n",
            "    📉 Learning rate reduced: 6.11e-05 → 6.22e-05\n",
            "  Epoch 31: Train=5.7015%, Val=3.9774%, LR=0.000066, Time=33.5s\n",
            "    📉 Learning rate reduced: 6.22e-05 → 6.61e-05\n",
            "    📉 Learning rate reduced: 6.61e-05 → 6.42e-05\n",
            "    📉 Learning rate reduced: 6.42e-05 → 6.13e-05\n",
            "  Epoch 34: Train=5.5383%, Val=3.9581%, LR=0.000066, Time=34.0s\n",
            "    📉 Learning rate reduced: 6.13e-05 → 6.64e-05\n",
            "    📉 Learning rate reduced: 6.64e-05 → 6.59e-05\n",
            "✅ Fold 2 Seed 456 complete: Best Val MAPE = 3.9581% in 23.2min\n",
            "📈 Fold 2 ensemble results:\n",
            "   Mean MAPE: 4.0084% ± 0.0377%\n",
            "   Best MAPE: 3.9581%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 68.9 minutes\n",
            "\n",
            "🎯 Training Fold 3 with 3 seeds\n",
            "🚀 Training Fold 3 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=81.5880%, Val=26.7393%, LR=0.000007, Time=33.9s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 7.35e-06\n",
            "  Epoch  2: Train=26.2124%, Val=14.6405%, LR=0.000087, Time=33.3s\n",
            "    📉 Learning rate reduced: 7.35e-06 → 8.74e-05\n",
            "  Epoch  3: Train=19.4166%, Val=13.2734%, LR=0.000094, Time=42.2s\n",
            "    📉 Learning rate reduced: 8.74e-05 → 9.36e-05\n",
            "  Epoch  4: Train=14.0734%, Val=7.6640%, LR=0.000014, Time=42.6s\n",
            "    📉 Learning rate reduced: 9.36e-05 → 1.37e-05\n",
            "  Epoch  5: Train=12.0248%, Val=7.1550%, LR=0.000019, Time=42.4s\n",
            "    📉 Learning rate reduced: 1.37e-05 → 1.95e-05\n",
            "  Epoch  6: Train=11.4022%, Val=6.6474%, LR=0.000026, Time=42.9s\n",
            "    📉 Learning rate reduced: 1.95e-05 → 2.60e-05\n",
            "  Epoch  7: Train=10.9515%, Val=6.4771%, LR=0.000028, Time=42.9s\n",
            "    📉 Learning rate reduced: 2.60e-05 → 2.83e-05\n",
            "  Epoch  8: Train=10.4258%, Val=6.3702%, LR=0.000030, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.83e-05 → 2.98e-05\n",
            "  Epoch  9: Train=10.0615%, Val=6.1159%, LR=0.000033, Time=42.6s\n",
            "    📉 Learning rate reduced: 2.98e-05 → 3.35e-05\n",
            "  Epoch 10: Train=9.4461%, Val=5.8348%, LR=0.000038, Time=42.6s\n",
            "    📉 Learning rate reduced: 3.35e-05 → 3.77e-05\n",
            "  Epoch 11: Train=9.2199%, Val=6.0009%, LR=0.000035, Time=42.2s\n",
            "    📉 Learning rate reduced: 3.77e-05 → 3.52e-05\n",
            "  Epoch 12: Train=8.7591%, Val=5.4832%, LR=0.000043, Time=34.0s\n",
            "    📉 Learning rate reduced: 3.52e-05 → 4.30e-05\n",
            "  Epoch 13: Train=8.3477%, Val=5.1789%, LR=0.000048, Time=42.1s\n",
            "    📉 Learning rate reduced: 4.30e-05 → 4.77e-05\n",
            "    📉 Learning rate reduced: 4.77e-05 → 4.56e-05\n",
            "  Epoch 15: Train=7.8461%, Val=4.9428%, LR=0.000051, Time=33.4s\n",
            "    📉 Learning rate reduced: 4.56e-05 → 5.14e-05\n",
            "  Epoch 16: Train=7.5722%, Val=4.8261%, LR=0.000053, Time=42.3s\n",
            "    📉 Learning rate reduced: 5.14e-05 → 5.32e-05\n",
            "    📉 Learning rate reduced: 5.32e-05 → 4.92e-05\n",
            "    📉 Learning rate reduced: 4.92e-05 → 4.99e-05\n",
            "  Epoch 19: Train=7.0190%, Val=4.4441%, LR=0.000059, Time=33.6s\n",
            "    📉 Learning rate reduced: 4.99e-05 → 5.91e-05\n",
            "    📉 Learning rate reduced: 5.91e-05 → 5.58e-05\n",
            "  Epoch 21: Train=6.6287%, Val=4.5129%, LR=0.000058, Time=33.9s\n",
            "    📉 Learning rate reduced: 5.58e-05 → 5.80e-05\n",
            "    📉 Learning rate reduced: 5.80e-05 → 5.83e-05\n",
            "  Epoch 23: Train=6.4564%, Val=4.3623%, LR=0.000060, Time=34.3s\n",
            "    📉 Learning rate reduced: 5.83e-05 → 6.04e-05\n",
            "    📉 Learning rate reduced: 6.04e-05 → 5.78e-05\n",
            "  Epoch 25: Train=6.1068%, Val=4.2872%, LR=0.000061, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.78e-05 → 6.15e-05\n",
            "  Epoch 26: Train=5.9998%, Val=4.1068%, LR=0.000064, Time=42.4s\n",
            "    📉 Learning rate reduced: 6.15e-05 → 6.42e-05\n",
            "  Epoch 27: Train=5.9848%, Val=4.0880%, LR=0.000064, Time=42.0s\n",
            "    📉 Learning rate reduced: 6.42e-05 → 6.45e-05\n",
            "    📉 Learning rate reduced: 6.45e-05 → 6.37e-05\n",
            "    📉 Learning rate reduced: 6.37e-05 → 6.32e-05\n",
            "    📉 Learning rate reduced: 6.32e-05 → 6.32e-05\n",
            "  Epoch 31: Train=5.5448%, Val=4.1735%, LR=0.000063, Time=33.8s\n",
            "    📉 Learning rate reduced: 6.32e-05 → 6.32e-05\n",
            "    📉 Learning rate reduced: 6.32e-05 → 6.35e-05\n",
            "  Epoch 33: Train=5.4017%, Val=3.9304%, LR=0.000067, Time=33.2s\n",
            "    📉 Learning rate reduced: 6.35e-05 → 6.68e-05\n",
            "    📉 Learning rate reduced: 6.68e-05 → 5.88e-05\n",
            "  Epoch 35: Train=5.3745%, Val=3.8525%, LR=0.000068, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.88e-05 → 6.80e-05\n",
            "✅ Fold 3 Seed 42 complete: Best Val MAPE = 3.8525% in 22.7min\n",
            "🚀 Training Fold 3 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=60.3588%, Val=26.7895%, LR=0.000007, Time=42.1s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 7.16e-06\n",
            "  Epoch  2: Train=25.6058%, Val=14.4594%, LR=0.000088, Time=35.0s\n",
            "    📉 Learning rate reduced: 7.16e-06 → 8.83e-05\n",
            "  Epoch  3: Train=19.4535%, Val=11.7630%, LR=0.000098, Time=42.5s\n",
            "    📉 Learning rate reduced: 8.83e-05 → 9.81e-05\n",
            "  Epoch  4: Train=14.3033%, Val=10.0300%, LR=0.000100, Time=42.9s\n",
            "    📉 Learning rate reduced: 9.81e-05 → 1.00e-04\n",
            "  Epoch  5: Train=11.8836%, Val=7.7715%, LR=0.000013, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.26e-05\n",
            "  Epoch  6: Train=10.4272%, Val=6.6812%, LR=0.000026, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.26e-05 → 2.56e-05\n",
            "    📉 Learning rate reduced: 2.56e-05 → 2.39e-05\n",
            "  Epoch  8: Train=9.6900%, Val=6.3922%, LR=0.000030, Time=34.1s\n",
            "    📉 Learning rate reduced: 2.39e-05 → 2.95e-05\n",
            "  Epoch  9: Train=9.3976%, Val=5.9040%, LR=0.000037, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.95e-05 → 3.66e-05\n",
            "    📉 Learning rate reduced: 3.66e-05 → 2.94e-05\n",
            "  Epoch 11: Train=8.4578%, Val=5.4039%, LR=0.000044, Time=33.4s\n",
            "    📉 Learning rate reduced: 2.94e-05 → 4.42e-05\n",
            "    📉 Learning rate reduced: 4.42e-05 → 3.23e-05\n",
            "  Epoch 13: Train=7.9005%, Val=5.2073%, LR=0.000047, Time=33.6s\n",
            "    📉 Learning rate reduced: 3.23e-05 → 4.73e-05\n",
            "    📉 Learning rate reduced: 4.73e-05 → 3.75e-05\n",
            "  Epoch 15: Train=7.4232%, Val=5.1761%, LR=0.000048, Time=33.6s\n",
            "    📉 Learning rate reduced: 3.75e-05 → 4.78e-05\n",
            "  Epoch 16: Train=7.2933%, Val=5.9033%, LR=0.000037, Time=42.4s\n",
            "    📉 Learning rate reduced: 4.78e-05 → 3.66e-05\n",
            "    📉 Learning rate reduced: 3.66e-05 → 4.63e-05\n",
            "  Epoch 18: Train=6.9625%, Val=4.9220%, LR=0.000052, Time=33.5s\n",
            "    📉 Learning rate reduced: 4.63e-05 → 5.17e-05\n",
            "    📉 Learning rate reduced: 5.17e-05 → 5.03e-05\n",
            "  Epoch 20: Train=6.5725%, Val=4.4426%, LR=0.000059, Time=34.3s\n",
            "    📉 Learning rate reduced: 5.03e-05 → 5.91e-05\n",
            "  Epoch 21: Train=6.5553%, Val=4.8251%, LR=0.000053, Time=42.8s\n",
            "    📉 Learning rate reduced: 5.91e-05 → 5.32e-05\n",
            "    📉 Learning rate reduced: 5.32e-05 → 5.17e-05\n",
            "    📉 Learning rate reduced: 5.17e-05 → 5.64e-05\n",
            "  Epoch 24: Train=6.3426%, Val=4.3709%, LR=0.000060, Time=33.1s\n",
            "    📉 Learning rate reduced: 5.64e-05 → 6.02e-05\n",
            "    📉 Learning rate reduced: 6.02e-05 → 4.86e-05\n",
            "  Epoch 26: Train=6.1057%, Val=4.1808%, LR=0.000063, Time=33.9s\n",
            "    📉 Learning rate reduced: 4.86e-05 → 6.31e-05\n",
            "    📉 Learning rate reduced: 6.31e-05 → 5.87e-05\n",
            "    📉 Learning rate reduced: 5.87e-05 → 4.99e-05\n",
            "    📉 Learning rate reduced: 4.99e-05 → 5.47e-05\n",
            "    📉 Learning rate reduced: 5.47e-05 → 5.08e-05\n",
            "  Epoch 31: Train=5.7200%, Val=4.2735%, LR=0.000062, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.08e-05 → 6.17e-05\n",
            "    📉 Learning rate reduced: 6.17e-05 → 6.07e-05\n",
            "    📉 Learning rate reduced: 6.07e-05 → 5.41e-05\n",
            "    📉 Learning rate reduced: 5.41e-05 → 5.55e-05\n",
            "    ⏹️  Early stopping at epoch 34\n",
            "✅ Fold 3 Seed 123 complete: Best Val MAPE = 4.1808% in 21.5min\n",
            "🚀 Training Fold 3 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=72.7602%, Val=27.7041%, LR=0.000004, Time=33.0s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 4.18e-06\n",
            "  Epoch  2: Train=27.3822%, Val=16.2072%, LR=0.000078, Time=34.5s\n",
            "    📉 Learning rate reduced: 4.18e-06 → 7.83e-05\n",
            "  Epoch  3: Train=20.9730%, Val=12.3205%, LR=0.000097, Time=42.4s\n",
            "    📉 Learning rate reduced: 7.83e-05 → 9.67e-05\n",
            "  Epoch  4: Train=14.9415%, Val=10.3527%, LR=0.000100, Time=42.1s\n",
            "    📉 Learning rate reduced: 9.67e-05 → 9.99e-05\n",
            "  Epoch  5: Train=11.9265%, Val=8.2920%, LR=0.000008, Time=42.2s\n",
            "    📉 Learning rate reduced: 9.99e-05 → 7.96e-06\n",
            "  Epoch  6: Train=10.7587%, Val=6.7052%, LR=0.000025, Time=42.8s\n",
            "    📉 Learning rate reduced: 7.96e-06 → 2.52e-05\n",
            "  Epoch  7: Train=10.4145%, Val=6.4062%, LR=0.000029, Time=42.9s\n",
            "    📉 Learning rate reduced: 2.52e-05 → 2.93e-05\n",
            "    📉 Learning rate reduced: 2.93e-05 → 2.59e-05\n",
            "  Epoch  9: Train=9.6468%, Val=6.1232%, LR=0.000033, Time=33.5s\n",
            "    📉 Learning rate reduced: 2.59e-05 → 3.34e-05\n",
            "  Epoch 10: Train=9.3276%, Val=5.7577%, LR=0.000039, Time=42.4s\n",
            "    📉 Learning rate reduced: 3.34e-05 → 3.88e-05\n",
            "  Epoch 11: Train=9.0936%, Val=5.5998%, LR=0.000041, Time=42.5s\n",
            "    📉 Learning rate reduced: 3.88e-05 → 4.12e-05\n",
            "    📉 Learning rate reduced: 4.12e-05 → 4.09e-05\n",
            "  Epoch 13: Train=8.2556%, Val=5.3775%, LR=0.000045, Time=33.5s\n",
            "    📉 Learning rate reduced: 4.09e-05 → 4.46e-05\n",
            "  Epoch 14: Train=7.8879%, Val=4.9711%, LR=0.000051, Time=42.7s\n",
            "    📉 Learning rate reduced: 4.46e-05 → 5.09e-05\n",
            "    📉 Learning rate reduced: 5.09e-05 → 5.02e-05\n",
            "  Epoch 16: Train=7.3359%, Val=4.8659%, LR=0.000053, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.02e-05 → 5.26e-05\n",
            "  Epoch 17: Train=7.2511%, Val=4.8518%, LR=0.000053, Time=42.6s\n",
            "    📉 Learning rate reduced: 5.26e-05 → 5.28e-05\n",
            "  Epoch 18: Train=6.9957%, Val=4.5699%, LR=0.000057, Time=42.2s\n",
            "    📉 Learning rate reduced: 5.28e-05 → 5.72e-05\n",
            "  Epoch 19: Train=6.8405%, Val=4.3633%, LR=0.000060, Time=42.5s\n",
            "    📉 Learning rate reduced: 5.72e-05 → 6.03e-05\n",
            "    📉 Learning rate reduced: 6.03e-05 → 5.16e-05\n",
            "  Epoch 21: Train=6.5232%, Val=4.4424%, LR=0.000059, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.16e-05 → 5.91e-05\n",
            "  Epoch 22: Train=6.3228%, Val=4.2852%, LR=0.000062, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.91e-05 → 6.15e-05\n",
            "    📉 Learning rate reduced: 6.15e-05 → 5.32e-05\n",
            "  Epoch 24: Train=6.4377%, Val=4.1493%, LR=0.000064, Time=33.7s\n",
            "    📉 Learning rate reduced: 5.32e-05 → 6.36e-05\n",
            "    📉 Learning rate reduced: 6.36e-05 → 5.96e-05\n",
            "  Epoch 26: Train=5.9704%, Val=4.0887%, LR=0.000064, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.96e-05 → 6.45e-05\n",
            "  Epoch 27: Train=5.9849%, Val=4.0561%, LR=0.000065, Time=42.4s\n",
            "    📉 Learning rate reduced: 6.45e-05 → 6.50e-05\n",
            "    📉 Learning rate reduced: 6.50e-05 → 5.89e-05\n",
            "    📉 Learning rate reduced: 5.89e-05 → 6.31e-05\n",
            "    📉 Learning rate reduced: 6.31e-05 → 6.09e-05\n",
            "  Epoch 31: Train=5.7382%, Val=4.1817%, LR=0.000063, Time=33.4s\n",
            "    📉 Learning rate reduced: 6.09e-05 → 6.31e-05\n",
            "    📉 Learning rate reduced: 6.31e-05 → 6.18e-05\n",
            "    📉 Learning rate reduced: 6.18e-05 → 6.34e-05\n",
            "    📉 Learning rate reduced: 6.34e-05 → 6.12e-05\n",
            "  Epoch 35: Train=5.4526%, Val=3.8113%, LR=0.000069, Time=33.2s\n",
            "    📉 Learning rate reduced: 6.12e-05 → 6.86e-05\n",
            "✅ Fold 3 Seed 456 complete: Best Val MAPE = 3.8113% in 22.7min\n",
            "📈 Fold 3 ensemble results:\n",
            "   Mean MAPE: 3.9482% ± 0.1653%\n",
            "   Best MAPE: 3.8113%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 67.2 minutes\n",
            "\n",
            "🎯 Training Fold 4 with 3 seeds\n",
            "🚀 Training Fold 4 with Seed 42\n",
            "✅ Deterministic training setup complete with seed 42\n",
            "  Epoch  1: Train=81.4287%, Val=28.7855%, LR=0.000002, Time=42.4s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.90e-06\n",
            "  Epoch  2: Train=26.6369%, Val=14.7210%, LR=0.000087, Time=34.9s\n",
            "    📉 Learning rate reduced: 1.90e-06 → 8.70e-05\n",
            "  Epoch  3: Train=20.8697%, Val=11.3495%, LR=0.000099, Time=42.7s\n",
            "    📉 Learning rate reduced: 8.70e-05 → 9.89e-05\n",
            "  Epoch  4: Train=14.7348%, Val=9.1876%, LR=0.000003, Time=42.2s\n",
            "    📉 Learning rate reduced: 9.89e-05 → 2.60e-06\n",
            "  Epoch  5: Train=12.4335%, Val=7.1810%, LR=0.000019, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.60e-06 → 1.92e-05\n",
            "  Epoch  6: Train=11.9466%, Val=6.9593%, LR=0.000022, Time=42.1s\n",
            "    📉 Learning rate reduced: 1.92e-05 → 2.19e-05\n",
            "  Epoch  7: Train=11.4745%, Val=6.7135%, LR=0.000025, Time=42.5s\n",
            "    📉 Learning rate reduced: 2.19e-05 → 2.51e-05\n",
            "  Epoch  8: Train=10.9205%, Val=6.5395%, LR=0.000027, Time=42.6s\n",
            "    📉 Learning rate reduced: 2.51e-05 → 2.75e-05\n",
            "  Epoch  9: Train=10.5694%, Val=6.3020%, LR=0.000031, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.75e-05 → 3.08e-05\n",
            "    📉 Learning rate reduced: 3.08e-05 → 2.88e-05\n",
            "  Epoch 11: Train=9.5917%, Val=5.8518%, LR=0.000037, Time=33.1s\n",
            "    📉 Learning rate reduced: 2.88e-05 → 3.74e-05\n",
            "  Epoch 12: Train=9.3040%, Val=5.5737%, LR=0.000042, Time=42.8s\n",
            "    📉 Learning rate reduced: 3.74e-05 → 4.16e-05\n",
            "  Epoch 13: Train=8.8158%, Val=5.5556%, LR=0.000042, Time=42.7s\n",
            "    📉 Learning rate reduced: 4.16e-05 → 4.19e-05\n",
            "    📉 Learning rate reduced: 4.19e-05 → 3.87e-05\n",
            "  Epoch 15: Train=8.0956%, Val=5.0777%, LR=0.000049, Time=33.9s\n",
            "    📉 Learning rate reduced: 3.87e-05 → 4.93e-05\n",
            "  Epoch 16: Train=7.9072%, Val=4.8272%, LR=0.000053, Time=42.1s\n",
            "    📉 Learning rate reduced: 4.93e-05 → 5.32e-05\n",
            "  Epoch 17: Train=7.5171%, Val=4.7700%, LR=0.000054, Time=42.5s\n",
            "    📉 Learning rate reduced: 5.32e-05 → 5.41e-05\n",
            "    📉 Learning rate reduced: 5.41e-05 → 4.41e-05\n",
            "  Epoch 19: Train=7.1447%, Val=4.6148%, LR=0.000056, Time=33.4s\n",
            "    📉 Learning rate reduced: 4.41e-05 → 5.65e-05\n",
            "  Epoch 20: Train=6.9265%, Val=4.4404%, LR=0.000059, Time=42.3s\n",
            "    📉 Learning rate reduced: 5.65e-05 → 5.92e-05\n",
            "  Epoch 21: Train=6.6814%, Val=4.4401%, LR=0.000059, Time=42.1s\n",
            "    📉 Learning rate reduced: 5.92e-05 → 5.92e-05\n",
            "    📉 Learning rate reduced: 5.92e-05 → 5.42e-05\n",
            "    📉 Learning rate reduced: 5.42e-05 → 5.68e-05\n",
            "    📉 Learning rate reduced: 5.68e-05 → 5.71e-05\n",
            "  Epoch 25: Train=6.1961%, Val=4.1666%, LR=0.000063, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.71e-05 → 6.33e-05\n",
            "  Epoch 26: Train=6.1478%, Val=4.1235%, LR=0.000064, Time=42.1s\n",
            "    📉 Learning rate reduced: 6.33e-05 → 6.40e-05\n",
            "    📉 Learning rate reduced: 6.40e-05 → 6.15e-05\n",
            "  Epoch 28: Train=6.0875%, Val=4.0840%, LR=0.000065, Time=33.0s\n",
            "    📉 Learning rate reduced: 6.15e-05 → 6.45e-05\n",
            "    📉 Learning rate reduced: 6.45e-05 → 6.30e-05\n",
            "    📉 Learning rate reduced: 6.30e-05 → 6.34e-05\n",
            "  Epoch 31: Train=5.6621%, Val=4.2602%, LR=0.000062, Time=34.0s\n",
            "    📉 Learning rate reduced: 6.34e-05 → 6.19e-05\n",
            "  Epoch 32: Train=5.6667%, Val=4.0087%, LR=0.000066, Time=33.6s\n",
            "    📉 Learning rate reduced: 6.19e-05 → 6.57e-05\n",
            "    📉 Learning rate reduced: 6.57e-05 → 6.13e-05\n",
            "    📉 Learning rate reduced: 6.13e-05 → 5.68e-05\n",
            "  Epoch 35: Train=5.3891%, Val=3.8174%, LR=0.000068, Time=33.5s\n",
            "    📉 Learning rate reduced: 5.68e-05 → 6.85e-05\n",
            "✅ Fold 4 Seed 42 complete: Best Val MAPE = 3.8174% in 23.2min\n",
            "🚀 Training Fold 4 with Seed 123\n",
            "✅ Deterministic training setup complete with seed 123\n",
            "  Epoch  1: Train=59.7339%, Val=24.9223%, LR=0.000016, Time=42.1s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 1.59e-05\n",
            "  Epoch  2: Train=23.9064%, Val=13.3841%, LR=0.000093, Time=34.1s\n",
            "    📉 Learning rate reduced: 1.59e-05 → 9.32e-05\n",
            "  Epoch  3: Train=18.1310%, Val=10.9662%, LR=0.000099, Time=42.9s\n",
            "    📉 Learning rate reduced: 9.32e-05 → 9.94e-05\n",
            "  Epoch  4: Train=13.6581%, Val=8.9286%, LR=0.000004, Time=42.7s\n",
            "    📉 Learning rate reduced: 9.94e-05 → 3.78e-06\n",
            "  Epoch  5: Train=11.9599%, Val=7.4544%, LR=0.000016, Time=42.6s\n",
            "    📉 Learning rate reduced: 3.78e-06 → 1.60e-05\n",
            "  Epoch  6: Train=11.4407%, Val=7.3494%, LR=0.000017, Time=42.7s\n",
            "    📉 Learning rate reduced: 1.60e-05 → 1.72e-05\n",
            "  Epoch  7: Train=10.9602%, Val=6.7513%, LR=0.000025, Time=42.4s\n",
            "    📉 Learning rate reduced: 1.72e-05 → 2.46e-05\n",
            "  Epoch  8: Train=10.5753%, Val=6.7244%, LR=0.000025, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.46e-05 → 2.50e-05\n",
            "  Epoch  9: Train=10.2406%, Val=6.5260%, LR=0.000028, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.50e-05 → 2.77e-05\n",
            "  Epoch 10: Train=9.8148%, Val=6.4094%, LR=0.000029, Time=42.8s\n",
            "    📉 Learning rate reduced: 2.77e-05 → 2.93e-05\n",
            "  Epoch 11: Train=9.3563%, Val=5.9932%, LR=0.000035, Time=42.3s\n",
            "    📉 Learning rate reduced: 2.93e-05 → 3.53e-05\n",
            "    📉 Learning rate reduced: 3.53e-05 → 3.34e-05\n",
            "  Epoch 13: Train=8.6955%, Val=5.7700%, LR=0.000039, Time=33.5s\n",
            "    📉 Learning rate reduced: 3.34e-05 → 3.86e-05\n",
            "  Epoch 14: Train=8.3086%, Val=5.6276%, LR=0.000041, Time=42.4s\n",
            "    📉 Learning rate reduced: 3.86e-05 → 4.08e-05\n",
            "    📉 Learning rate reduced: 4.08e-05 → 3.83e-05\n",
            "  Epoch 16: Train=7.9139%, Val=5.0242%, LR=0.000050, Time=33.3s\n",
            "    📉 Learning rate reduced: 3.83e-05 → 5.01e-05\n",
            "  Epoch 17: Train=7.5574%, Val=4.9832%, LR=0.000051, Time=42.5s\n",
            "    📉 Learning rate reduced: 5.01e-05 → 5.08e-05\n",
            "    📉 Learning rate reduced: 5.08e-05 → 4.71e-05\n",
            "    📉 Learning rate reduced: 4.71e-05 → 4.85e-05\n",
            "  Epoch 20: Train=6.8672%, Val=4.8252%, LR=0.000053, Time=33.9s\n",
            "    📉 Learning rate reduced: 4.85e-05 → 5.32e-05\n",
            "  Epoch 21: Train=6.7911%, Val=4.7200%, LR=0.000055, Time=42.6s\n",
            "    📉 Learning rate reduced: 5.32e-05 → 5.48e-05\n",
            "    📉 Learning rate reduced: 5.48e-05 → 5.31e-05\n",
            "    📉 Learning rate reduced: 5.31e-05 → 5.27e-05\n",
            "  Epoch 24: Train=6.4115%, Val=4.6563%, LR=0.000056, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.27e-05 → 5.58e-05\n",
            "  Epoch 25: Train=6.3457%, Val=4.5822%, LR=0.000057, Time=41.8s\n",
            "    📉 Learning rate reduced: 5.58e-05 → 5.70e-05\n",
            "  Epoch 26: Train=6.3544%, Val=4.3473%, LR=0.000061, Time=42.4s\n",
            "    📉 Learning rate reduced: 5.70e-05 → 6.06e-05\n",
            "    📉 Learning rate reduced: 6.06e-05 → 5.51e-05\n",
            "    📉 Learning rate reduced: 5.51e-05 → 5.87e-05\n",
            "    📉 Learning rate reduced: 5.87e-05 → 4.98e-05\n",
            "    📉 Learning rate reduced: 4.98e-05 → 5.72e-05\n",
            "  Epoch 31: Train=5.8470%, Val=4.2064%, LR=0.000063, Time=33.3s\n",
            "    📉 Learning rate reduced: 5.72e-05 → 6.27e-05\n",
            "    📉 Learning rate reduced: 6.27e-05 → 5.64e-05\n",
            "    📉 Learning rate reduced: 5.64e-05 → 5.42e-05\n",
            "    📉 Learning rate reduced: 5.42e-05 → 4.92e-05\n",
            "    📉 Learning rate reduced: 4.92e-05 → 6.26e-05\n",
            "✅ Fold 4 Seed 123 complete: Best Val MAPE = 4.2064% in 23.1min\n",
            "🚀 Training Fold 4 with Seed 456\n",
            "✅ Deterministic training setup complete with seed 456\n",
            "  Epoch  1: Train=72.6435%, Val=26.5515%, LR=0.000008, Time=34.1s\n",
            "    📉 Learning rate reduced: 1.00e-04 → 8.09e-06\n",
            "  Epoch  2: Train=26.5890%, Val=15.1771%, LR=0.000085, Time=34.1s\n",
            "    📉 Learning rate reduced: 8.09e-06 → 8.45e-05\n",
            "  Epoch  3: Train=20.2260%, Val=11.5088%, LR=0.000099, Time=42.1s\n",
            "    📉 Learning rate reduced: 8.45e-05 → 9.86e-05\n",
            "  Epoch  4: Train=14.9842%, Val=8.9860%, LR=0.000003, Time=42.0s\n",
            "    📉 Learning rate reduced: 9.86e-05 → 3.49e-06\n",
            "  Epoch  5: Train=12.7214%, Val=7.2559%, LR=0.000018, Time=42.2s\n",
            "    📉 Learning rate reduced: 3.49e-06 → 1.83e-05\n",
            "  Epoch  6: Train=12.2340%, Val=7.1345%, LR=0.000020, Time=43.0s\n",
            "    📉 Learning rate reduced: 1.83e-05 → 1.97e-05\n",
            "  Epoch  7: Train=11.7078%, Val=6.7732%, LR=0.000024, Time=42.6s\n",
            "    📉 Learning rate reduced: 1.97e-05 → 2.43e-05\n",
            "  Epoch  8: Train=11.1348%, Val=6.7465%, LR=0.000025, Time=42.2s\n",
            "    📉 Learning rate reduced: 2.43e-05 → 2.47e-05\n",
            "  Epoch  9: Train=10.6478%, Val=6.3048%, LR=0.000031, Time=42.7s\n",
            "    📉 Learning rate reduced: 2.47e-05 → 3.08e-05\n",
            "  Epoch 10: Train=10.2592%, Val=6.0117%, LR=0.000035, Time=42.3s\n",
            "    📉 Learning rate reduced: 3.08e-05 → 3.50e-05\n",
            "  Epoch 11: Train=9.7501%, Val=5.8409%, LR=0.000038, Time=42.7s\n",
            "    📉 Learning rate reduced: 3.50e-05 → 3.76e-05\n",
            "    📉 Learning rate reduced: 3.76e-05 → 3.64e-05\n",
            "  Epoch 13: Train=8.9094%, Val=5.5530%, LR=0.000042, Time=33.5s\n",
            "    📉 Learning rate reduced: 3.64e-05 → 4.19e-05\n",
            "    📉 Learning rate reduced: 4.19e-05 → 3.89e-05\n",
            "  Epoch 15: Train=8.3336%, Val=5.4374%, LR=0.000044, Time=33.5s\n",
            "    📉 Learning rate reduced: 3.89e-05 → 4.37e-05\n",
            "  Epoch 16: Train=7.9001%, Val=5.1406%, LR=0.000048, Time=42.5s\n",
            "    📉 Learning rate reduced: 4.37e-05 → 4.83e-05\n",
            "  Epoch 17: Train=7.8486%, Val=4.9280%, LR=0.000052, Time=42.5s\n",
            "    📉 Learning rate reduced: 4.83e-05 → 5.16e-05\n",
            "  Epoch 18: Train=7.6150%, Val=4.8320%, LR=0.000053, Time=42.7s\n",
            "    📉 Learning rate reduced: 5.16e-05 → 5.31e-05\n",
            "  Epoch 19: Train=7.2234%, Val=4.7937%, LR=0.000054, Time=42.3s\n",
            "    📉 Learning rate reduced: 5.31e-05 → 5.37e-05\n",
            "  Epoch 20: Train=7.1728%, Val=4.6567%, LR=0.000056, Time=42.4s\n",
            "    📉 Learning rate reduced: 5.37e-05 → 5.58e-05\n",
            "  Epoch 21: Train=6.9780%, Val=4.9339%, LR=0.000052, Time=42.4s\n",
            "    📉 Learning rate reduced: 5.58e-05 → 5.15e-05\n",
            "  Epoch 22: Train=6.7307%, Val=4.3391%, LR=0.000061, Time=33.2s\n",
            "    📉 Learning rate reduced: 5.15e-05 → 6.07e-05\n",
            "    📉 Learning rate reduced: 6.07e-05 → 5.15e-05\n",
            "  Epoch 24: Train=6.4889%, Val=4.1403%, LR=0.000064, Time=33.6s\n",
            "    📉 Learning rate reduced: 5.15e-05 → 6.37e-05\n",
            "    📉 Learning rate reduced: 6.37e-05 → 5.83e-05\n",
            "  Epoch 26: Train=6.3263%, Val=4.1615%, LR=0.000063, Time=34.1s\n",
            "    📉 Learning rate reduced: 5.83e-05 → 6.34e-05\n",
            "    📉 Learning rate reduced: 6.34e-05 → 6.27e-05\n",
            "    📉 Learning rate reduced: 6.27e-05 → 6.12e-05\n",
            "    📉 Learning rate reduced: 6.12e-05 → 6.01e-05\n",
            "    📉 Learning rate reduced: 6.01e-05 → 6.36e-05\n",
            "  Epoch 31: Train=5.8023%, Val=3.9731%, LR=0.000066, Time=33.5s\n",
            "    📉 Learning rate reduced: 6.36e-05 → 6.62e-05\n",
            "    📉 Learning rate reduced: 6.62e-05 → 5.81e-05\n",
            "    📉 Learning rate reduced: 5.81e-05 → 5.53e-05\n",
            "  Epoch 34: Train=5.5944%, Val=3.8668%, LR=0.000068, Time=33.4s\n",
            "    📉 Learning rate reduced: 5.53e-05 → 6.78e-05\n",
            "    📉 Learning rate reduced: 6.78e-05 → 6.61e-05\n",
            "✅ Fold 4 Seed 456 complete: Best Val MAPE = 3.8668% in 23.1min\n",
            "📈 Fold 4 ensemble results:\n",
            "   Mean MAPE: 3.9636% ± 0.1729%\n",
            "   Best MAPE: 3.8174%\n",
            "   Seeds: [42, 123, 456]\n",
            "   Time: 69.5 minutes\n",
            "\n",
            "============================================================\n",
            "📊 COMPREHENSIVE ENSEMBLE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "📋 Individual Fold Analysis:\n",
            "Fold   Best     Mean     Std      Seeds       \n",
            "--------------------------------------------------\n",
            "0      3.8925   3.9710   0.0957   3           \n",
            "1      3.8839   3.9522   0.0486   3           \n",
            "2      3.9581   4.0084   0.0377   3           \n",
            "3      3.8113   3.9482   0.1653   3           \n",
            "4      3.8174   3.9636   0.1729   3           \n",
            "\n",
            "🎯 Overall Performance Summary:\n",
            "   Total models trained: 15\n",
            "   Overall mean MAPE: 3.9687% ± 0.1204%\n",
            "   Best single model: 3.8113%\n",
            "   Expected ensemble MAPE: 3.9687%\n",
            "   Total training time: 6.64 hours\n",
            "   Average time per model: 26.5 minutes\n",
            "\n",
            "📈 Performance Analysis:\n",
            "   Best single fold performance: 3.8113%\n",
            "   Mean of best fold performances: 3.8726%\n",
            "   Stability improvement: 0.1204% standard deviation\n",
            "\n",
            "🏆 Competition Analysis:\n",
            "   Models below 3.0% target: 0/15\n",
            "   Success rate: 0.0%\n",
            "   🎯 Gap to target: 0.8113%\n",
            "💾 Results saved to:\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/ensemble_results.json\n",
            "   /content/drive/MyDrive/ThinkOnward/Result/Phase2c/training_summary.txt\n",
            "\n",
            "✅ Phase 2c Training Complete!\n",
            "Ready for diffusion model development in Phase 3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN1SnC5PfvfBY+3Cz5+2Ody",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}